---
site: bookdown::bookdown_site
title: 'Comparison of Multivariate Estimation Methods'
date: "`r Sys.Date()`"
author:
  - name: Raju Rimal
    email: raju.rimal@nmbu.no
    affiliation: KBM
    footnote: Corresponding Author
  - name: Trygve Almøy
    email: trygve.almoy@nmbu.no
    affiliation: KBM
  - name: Solve Sæbø
    email: solve.sabo@nmbu.no
    affiliation: NMBU
address:
  - code: KBM
    address: Faculty of Chemistry and Bioinformatics, Norwegian University of Life Sciences, Ås, Norway
  - code: NMBU
    address: Prorector, Norwegian University of Life Sciences, Ås, Norway
classoption: ["12pt", "3p", "authoryear"]
monofont: 'sourcecodepro'
monofontoptions: "Scale=0.7"
colorlinks: true
linespacing: 1.5
fontfamily: mathpazo
use-landscape-page: yes
tables: yes
bibliography: 'ref-db.bib'
biblio-title: References
biblio-style: elsarticle-harv
link-citations: true
github-repo: therimalaya/04-estimation-comparison
always_allow_html: true
url: 'http\://therimalaya.github.io/04-estimation-comparison'
knit: 'bookdown::render_book'
review: yes
keywords: ['model-comparison ', 'multi-response ', 'simrel ', 'estimation ', 'estimation error ', 'meta modeling ']
abstract: |
  Prediction performance often does not reflect the estimation behaviour of a method. High error in estimation not necessarily results in high prediction error but can lead to an unreliable prediction when test data are in a different direction than the training data. In addition, the effect of a variable becomes unstable and can not be interpreted in such situations. Many research fields are more interested in these estimates than performing prediction. This study compares some newly-developed (envelope) and well-established (PCR, PLS) estimation methods using simulated data with specifically designed properties such as multicollinearity, the correlation between multiple responses and position of principal components of predictors that are relevant for the response. This study aims to give some insights into these methods and help the researchers to understand and use them for further study. _Write some specifics from the results to show what we have found._
editor_options:
  chunk_output_type: console
---

```{r, echo = FALSE, warning=FALSE, message=FALSE, cache=FALSE}
options(digits = 3, scipen = 999)
source("scripts/00-function.r")
source("scripts/01-setup.r")
source("scripts/03-collection.r")
knitr::opts_chunk$set(comment = NULL, out.width = "100%", echo = FALSE)
if (knitr::is_html_output()) {
  knitr::opts_chunk$set(
    dev = c('svg', 'pdf'),
    fig.retina = 2,
    fig.pos='!htb'
    )
}
```

# Introduction #

Estimation of parameters in a regression model is an integral part of many research study. Research fields such as social science, econometrics, psychology and medical study are more interested in measuring the impact of certain indicator or variable rather than performing prediction. Such studies have a large influence on people’s perception and also help in policy making and decisions.

Technology has facilitated researcher to collect a large amount of data however often times, such data either contains irrelevant information or are highly collinear. Researchers are devising new estimators to extract information and identify their inter-relationship. Some estimators are robust towards fixing the multicollinearity problem while some are targeted to model only the relevant information contained in the response variable.

This study extends the [@rimal2019pred] and compares some well-established estimators such as Principal Components Analysis (PCA), Partial Least Squares (PLS) together with two new methods based on envelope estimation: Envelope estimation in predictor space (Xenv) [@cook2010envelope] and simultaneous estimation of envelope (Senv) [@cook2015simultaneous]. The estimation process of these methods is discussed in [Methods] section. The comparison tests the estimation performance of these methods using multi-response simulated data from a linear model with controlled properties. The properties include the number of predictors, level of multicollinearity, the correlation between different response variables and the position of relevant predictor components. These properties are explained in [Experimental Design] section together with the strategy behind the simulation and data model.

# Simulation Model #

* Reduction of the regression model
  + Include the figure from previous paper
* How the covariance and coefficients are related
* From the construction of the covariance matrix of latent variables to the simulated data
* How and what simulation parameters are related to properties of data

# Estimation Methods #

A regression model is written as,

\begin{equation}
\underset{(1\times m)}{\mathbf{y}} =
  \underset{(1\times p)(p\times m)}
    {\mathbf{x}\boldsymbol{\beta}} +
  \underset{(1 \times m)}{\boldsymbol{\varepsilon}}
(\#eq:reg-model)
\end{equation}
where $\mathbf{y}$ is a vector of $m$ responses measured about their means, $\mathbf{x}$ is a vector of $p$ predictors measured about their means, $\boldsymbol{\beta}$ is a matrix of regression coefficients and $\boldsymbol{\varepsilon}$ is a vector of independent error terms with constant variance $\boldsymbol{\Sigma}_{y|x}$. In ordinary least squares, coefficient $\boldsymbol{\beta}$ is estimated as,

\begin{equation}
\underset{(p\times m)}{\boldsymbol{\hat{\beta}}} =
  \left(\underset{(p\times n)}{\mathbf{x}^t}
  \underset{(n\times p)}{\mathbf{x}}\right)^{-1}
  \underset{(p \times n)(n \times m)}{\mathbf{x}^t\mathbf{y}}
(\#eq:reg-beta)
\end{equation}

## Principal Components Regression ##

Principal Components are new set of variables from the transformation of original dataset such that they are uncorrelated with each other and the variation in the original data are ordered from first to last of these new variables. Let us define a transformation of $\mathbf{x}$ as,

\begin{equation}
\underset{(1\times p)}{\mathbf{x}} =
  \underset{(1 \times k)}{\mathbf{z}}
  \underset{(k \times p)}{\mathbf{R}^t}
(\#eq:x2z)
\end{equation}
where $\mathbf{R}$ is the eigenvectors corresponding to the covariance of
$\mathbf{x}$ and $\mathbf{z}$ are the principal components. A regression model  can be defined in terms of $\mathbf{z}$ as,

\begin{equation}
\underset{(1 \times m)}{\mathbf{y}} =
  \underset{(1 \times k)}{\mathbf{z}}
    \underset{(k \times m)}{\boldsymbol{\alpha}} +
  \underset{(1 \times m)}{\boldsymbol{\varepsilon}}
(\#eq:latent-model)
\end{equation}

Since the variation is ordered in $z$, only $k\le p$ columns of $z$ are used so that $p-k$ uninformative components are not used for modeling. The regression coefficient of \@ref(eq:latent-model) can be estimated as,

\begin{equation}
\underset{(k\times m)}{\boldsymbol{\hat{\alpha}}} =
  \left(\underset{(k \times n)}{\mathbf{z}^t}
    \underset{(n \times k)}{\mathbf{z}}\right)^{-1}
  \underset{(k \times n)(n \times m)}{\mathbf{z}^t\mathbf{y}}
(\#eq:reg-alpha)
\end{equation}

Using \@ref(eq:x2z) in \@ref(eq:reg-beta), we get,

$$
\begin{aligned}
\underset{(p\times m)}{\boldsymbol{\hat{\beta}}} =
  \left[
    \underset{(p\times k)(k \times n)}{\mathbf{R}\mathbf{z}^t}
    \underset{(n\times k)(k \times p)}{\mathbf{z} \mathbf{R}^t}
  \right]^{-1}
  \underset{(p\times k)(k \times n)}{\mathbf{R}\mathbf{z}^t}
  \underset{(n\times m)}{y}
\end{aligned}
$$

## Partial Least Squares Regression ##
- How beta coefficients are constructed
- How it is dependent on the variance-covariance matrices
- In what way PLS1 and PLS2 differ

## Envelope Estimations ##
- How beta coefficients are constructed
- How it is dependent on the variance-covariance matrices
- In what way Xenv and Senv differ

# Experimental Design #
An R [@coreR2018] package `simrel` [@Rimal2018; @saebo2015simrel] is used to simulate the data for comparison. In the simulation the number of observation is fixed at $n = 100$ and following four simulation parameters are used to obtain the data with wide range of properties.

**Number of predictors:**
: In order to cover both tall $(n>p)$ and wide $(p>n)$ cases, $p=20$ and $p=250$ number of predictors are simulated.

**Multicollinearity in predictor variables:**
: A parameter `gamma` $(\gamma)$ in simulation controls the exponential decline of eigenvalues $(\lambda_i, i = 1, \ldots p)$ corresponding to predictor variables as,
  \begin{equation}
  \lambda_i = e^{-\gamma(i-1)}, \gamma > 0 \text{ and } i = 1, 2, \ldots p
  (\#eq:gamma)
  \end{equation}
: Two levels 0.2 and 0.8 of `gamma` are used for simulation so that level 0.2 simulates the data with low multicollinearity and 0.8 simulates the data with high multicollinearity.

**Position of relevant components:**
: Initial principal components of a non-singular covariance matrix are larger than the later one. If the principal components corresponding to predictors with larger variation is not relevant for a response, this will just increase noise in the model. Here we will use two different levels of position index of predictor components: a) 1, 2, 3, 4 and b) 5, 6, 7, 8. Predictor components irrelevant for a response makes prediction difficult [@Helland1994b]. When combined with multicollinearity, this factor can create both easy and difficult model for both estimation and prediction.

**Correlation in response variables:**
: Many estimators also uses the structure of response for their estimation. Here the correlation between the responses are varied through a simulation parameter `eta` $(\eta)$. The parameter controls the exponential decline of eigenvalues $\kappa_j, j = 1, \ldots m (\text{ number of responses})$ corresponding to response variables as,
\begin{equation}
\eta_i = e^{-\kappa(j-1)}, \kappa > 0 \text{ and } j = 1, 2, \ldots m
(\#eq:eta)
\end{equation}
: Four levels 0, 0.4, 0.8 and 1.2 of `eta` are used in the so that level 0 simulates the data with uncorrelated response variables while 1.2 simulates the highly correlated response variables.

```{r design-plot, fig.cap="Experimental Design of simulation parameters. Each point represents an unique data property.", echo = FALSE, fig.asp=0.4, fig.width=8}
design_chr %>%
    mutate(Design = row_number()) %>%
    ggplot(aes(eta, gamma)) +
    geom_point(shape=4) +
    ggrepel::geom_text_repel(
      aes(label = Design),
      nudge_x = 0.03, family = 'mono', fontface = "bold") +
    facet_grid(p ~ relpos, labeller=label_both) +
    scale_y_reverse(breaks = opts$gamma) +
    scale_x_continuous(breaks = opts$eta) +
    theme_minimal(base_size = 16, base_family = "mono") +
    theme(panel.grid.minor = element_blank(), text = element_text(face = "bold")) +
    coord_fixed(ratio=0.5)
```

Here we have assumed that there is only one informative response component. In the final dataset, all predictors together span the same space as the relevant predictor components and all response together span the same space as the one informative response component. In addition, coefficient of determination is fixed at 0.8 for all dataset.

A complete factorial design is adopted using different levels of factors discussed above to create 32 design (Figure \@ref(fig:design-plot)) each of which gives dataset with unique properties. From each  of these design and each estimation method, 50 different datasets are simulated so that each of them have same true population structure. In total, $`r length(mthds)` \times `r nrow(design)` \times 50$ i.e., `r length(mthds) * nrow(design) * 50` datasets are simulated.

(ref:cov-plot) Covariance between predictor components and response variables in population (top) and in the simulated data (bottom) for four different designs. The Bar in the background represents the variance of corresponding components.

```{r cov-plot, fig.asp=0.6, fig.cap="(ref:cov-plot)", fig.width=8}
selected_designs <- design %>%
  mutate(Design = row_number()) %>%
  filter(p == 20, eta == 0)
sobj_list <- lapply(1:nrow(selected_designs), function(i){
  set.seed(2019)
  selected_designs %>% select(-Design) %>% get_design(i) %>% simulate()
})
names(sobj_list) <- paste0("Design", selected_designs$Design)
sigma_zy_pop <- map_df(sobj_list, function(obj){
  obj %>%
    cov_mat(which = "zy", use_population = TRUE) %>%
    tidy_sigma() %>%
    abs_sigma()
}, .id = "Design")
sigma_zy_samp <- map_df(sobj_list, function(obj){
  obj %>%
    cov_mat(which = "zy", use_population = FALSE) %>%
    tidy_sigma() %>%
    abs_sigma()
}, .id = "Design")
sigma_zy <- bind_rows(
  Population = sigma_zy_pop,
  Sample = sigma_zy_samp,
  .id = "Type"
)
lambda_df <- bind_rows(
  Population = map_df(sobj_list, tidy_lambda, use_population = TRUE, .id = "Design"),
  Sample = map_df(sobj_list, tidy_lambda, use_population = FALSE, .id = "Design"),
  .id = "Type"
)
design_chr_selected <- selected_designs %>%
    select(relpos, gamma, Design) %>%
    modify_at("relpos", paste0) %>%
    mutate_at("relpos", ~gsub("list\\(c\\((.+)\\))", "\\1", ..1))
design_name <- paste0("Design", selected_designs$Design)
design_lbl <- with(design_chr_selected, {
  paste(design_name, map2_chr(relpos, gamma, paste, sep = " | "), sep = "\n")
})
names(design_lbl) <- design_name
ggplot(sigma_zy, aes(Predictor, Covariance, color = factor(Response))) +
  geom_bar(data = lambda_df, aes(x = Predictor, y = lambda),
           fill = "lightgrey",
           stat = "identity", inherit.aes = FALSE) +
  geom_point(size = rel(0.8)) +
  geom_line(size = rel(0.5)) +
  facet_grid(cols = vars(Design), rows = vars(Type),
             labeller = labeller(Design = design_lbl )) +
  theme(legend.position = "bottom") +
  labs(x = "Components",
       y = "Absolute Covariances",
       color = "Response Variable",
       title = "Covariance between Predictor Components and Responses",
       subtitle = "High/Low Multicollinearity with near/far relevant predictors") +
  scale_color_brewer(palette = "Set1")
```

The simulation properties are directly reflected in the simulated data. For example, in Figure \@ref(fig:cov-plot), design pairs 1 and 4 as well as 6 and 9 differs their properties only in terms of relevant predictor components while the design pairs 1 and 6 as well as 14 and 9 differs only in-terms of level of multicollinearity. The properties in population are also reflected in the simulated samples.

_\alert{May be we need to write few thing on how easy or difficult data are simulated with the interaction of these properties}_

```{r data-prep}
est_dta <- design_chr %>%
  select_if(function(x) n_distinct(x) > 1) %>%
  mutate(Design = as.character(1:n())) %>%
  mutate_at(vars(p, gamma, relpos, eta), as.factor) %>%
  right_join(est_error, by = "Design") %>%
  mutate_if(is.character, as.factor) %>%
  mutate_at("p", as.factor) %>%
  mutate(Response = paste0("Y", Response))
est_spread_df <- est_dta %>%
  as.data.frame() %>%
  select(-Design, -q) %>%
  spread(Response, Est_Error)
min_comp_stk <- est_dta %>%
  group_by(p, relpos, eta, gamma, Method, Tuning_Param, Response) %>%
  summarize(Est_Error = mean(Est_Error)) %>%
  group_by(p, relpos, eta, gamma, Method, Response) %>%
  summarize(Tuning_Param = Tuning_Param[which.min(Est_Error)])
est_min <- est_dta %>%
  select(-Design, -q) %>%
  semi_join(min_comp_stk, by = c(
    "p", "relpos", "eta", "gamma", "Method",
    "Tuning_Param", "Response"
  )) %>% select(-Tuning_Param) %>%
  spread(Response, Est_Error)
comp_min <- est_dta %>%
  group_by(p, relpos, eta, gamma, Method, Replication, Response) %>%
  summarize(Tuning_Param = Tuning_Param[which.min(Est_Error)]) %>%
  spread(Response, Tuning_Param)
```

# Basis of Comparison

The focus of this study is to extend the exploration of @rimal2019pred to compare the estimative performance of PCR, PLS1, PLS2, Xenv and Senv methods. The performance is measured on the basis of,

a) average estimation error of the method using arbitrary number of components
b) average number of components used by the methods to give minimum estimation error

Let us define the expected estimation error as,

\begin{equation}
\mathcal{EE}_{ijkl} =
  \mathsf{E}{\left[\left(\boldsymbol{\beta}_{ij} -
  \boldsymbol{\hat{\beta}_{ijkl}}\right)^t
  \left(\boldsymbol{\beta}_{ij} - \boldsymbol{\hat{\beta}_{ijkl}}\right)\right]}
(\#eq:est-error)
\end{equation}
for response $j = 1, \ldots 4$ in a given design $i=1, 2, \ldots 32$ and method $k=1(PCR), \ldots 5(Senv)$ using $l=0, \ldots 10$ number of components. Since both the expectation and the variance of $\hat{\boldsymbol{\beta}}$ are unknown, the prediction error are estimated using data from 50 replications as follows,

\begin{equation}
\widehat{\mathcal{EE}_{ijkl}} =
  \frac{1}{50}\sum_{r=0}^{50}{\left[\left(\boldsymbol{\beta}_{ij} -
  \boldsymbol{\hat{\beta}_{ijklr}}\right)^t
  \left(\boldsymbol{\beta}_{ij} - \boldsymbol{\hat{\beta}_{ijklr}}\right)\right]}
(\#eq:estimated-est-error)
\end{equation}
where, $\widehat{\mathcal{EE}_{ijkl}}$ is the estimated prediction error averaged over $r=50$ replicates.

## Data Preparation
A simulation strategy as in @rimal2019pred is adopted for preparation of data where the estimation error $(\beta_j - \hat{\beta}_j)^t(\beta_j - \hat{\beta}_j))$ is measured for each method, design and replication using number of components ranging from 0 to 10.

# Exploration

This section explores the variation in the _error dataset_ and the _component dataset_ for which we have used Principal Component Analysis (PCA). Let $t_u$ and $t_v$ be the principal component score sets corresponding to PCA run on the $u$ and $v$ matrices respectively. The scores density in Figure \@ref(fig:est-pca-hist-mthd-gamma-relpos) and Figure \@ref(fig:comp-pca-hist-mthd-gamma-relpos) correspond to the first principal component of $u$ and $v$, i.e. the first column of $t_u$ and $t_v$ respectively.

```{r pca}
est_pca <- with(est_min, prcomp(cbind(Y1, Y2, Y3, Y4)))
expl_var <- explvar(est_pca) %>% round(2)
est_dta_with_pc <- bind_cols(est_min, as.data.frame(scores(est_pca)[]))
comp_pca <- with(comp_min, prcomp(cbind(Y1, Y2, Y3, Y4)))
comp_expl_var <- explvar(comp_pca) %>% round(2)
comp_dta_with_pc <- bind_cols(comp_min, as.data.frame(scores(comp_pca)[]))
```

```{r pc-hist-plot-function}
pc_density_plot <- function(dta, expl_var, title) {
    dta %>%
        ggplot(aes(PC1, eta, fill = relpos)) +
        geom_density_ridges(
            scale = 1,
            alpha = 0.4, size = 0.25) +
        geom_density_ridges(
            scale = 0.95,
            alpha = 0.2, size = 0.25,
            stat = "binline", bins = 30) +
        facet_wrap(
            . ~ interaction(Method, paste0("gamma:", gamma), sep = "|"),
            scales = 'free_x', ncol = 5,
            labeller = labeller(gamma = label_both, p = label_both)) +
        theme_grey(base_family = 'mono') +
        theme(
            legend.position = "bottom",
            strip.text = element_text(family = "mono")) +
        labs(x = paste0("PC1(", expl_var[1], "%)")) +
        ggtitle(title) +
        scale_x_continuous(breaks = scales::pretty_breaks(3)) +
        scale_color_brewer(palette = "Set1") +
        scale_fill_brewer(palette = "Set1")
}
```

(ref:est-hist) Scores density corresponding to first principal component of _error dataset_ ($\mathbf{u}$) subdivided by `methods`, `gamma` and `eta` and grouped by `relpos`.

```{r est-pca-hist-mthd-gamma-relpos, message=FALSE, warning=FALSE, fig.cap="(ref:est-hist)", fig.pos="!htb"}
pc_density_plot(est_dta_with_pc, expl_var,
                title = "Density of PCA scores for error model")
```

The plot shows a clear difference between the effect of low and high multicollinearity in estimation error. In the case of low multicollinearity (`gamma: 0.2`), the estimation errors are smaller and have lesser variation compared to high multicollinearity (`gamma: 0.9`). High multicollinearity has a larger influence on all but noticeably in the methods based on envelopes. Some large estimation error in the envelope is more than 100 which in the case of other methods is less than 60.

Furthermore, the relevant predictor components, in general, has a noticeable effect on estimation error. When relevant predictors are at position 5, 6, 7, 8, the predictor components at 1, 2, 3, 4, which carry most of the variation, becomes irrelevant. These irrelevant components with large variation add noise to the model and consequently increases the estimation error. The effect intensifies on highly collinear predictors. Designs with high multicollinearity and relevant predictors at position 5, 6, 7, 8 are relatively difficult to model for all the methods. Although these difficult designs have a large effect on estimation error, their effect on prediction error is less influential [@rimal2019pred].

(ref:comp-hist) Score density corresponding to first principal component of _component dataset_ ($\mathbf{v}$) subdivided by `methods`, `gamma` and `eta` and grouped by `relpos`.

In the case of the _component dataset_ (Figure Above), PCR, PLS1 and PLS2 methods have used more components in the case of high multicollinearity compared to low. Surprisingly, the envelope methods (Senv and Xenv) mostly have used a distinctly lesser number of components in both the cases of multicollinearity compared to other methods.

The plot also shows that there is no clear effect due to the correlation of response variable on the number of components used to obtain minimum estimation error.

```{r comp-pca-hist-mthd-gamma-relpos, message=FALSE, warning=FALSE, fig.cap="(ref:comp-hist)", fig.pos="!htb"}
pc_density_plot(comp_dta_with_pc, comp_expl_var,
                title = "Density of PCA scores for component model")
```


A clear interaction between the position of relevant predictors and the multicollinearity visible in the plot suggest that the methods use a larger number of components when the relevant components are at position 5, 6, 7, 8. Additionally, the use of components escalate and the difference between the two levels of `relpos` becomes wider in the case of high multicollinearity in the model. Such performance is also seen the the case of prediction error (See @rimal2019pred) however the number of components used in that case is lesser than in this case. Envelope methods, however, have shown a distinct result in contrast to the other methods. Even when the relevant predictors are at position 5, 6, 7, 8, the envelope methods, in contrast to other methods, have used an almost similar number of components as in the case of relevant predictor at position 1, 2, 3, 4. This shows that the envelope methods identify the predictor space relevant to the response differently and with few numbers of latent components.

Following section explore the prediction and estimation error together with the regression coefficient of Simultaneous Envelope and Partial Least Squares for a design having high multicollinearity with predictor components at position 5, 6, 7, 8. Here we will use design with $n>p$ and no correlation between the response which corresponds to Design-9.

```{r}
load_local <- function(design, method) {
  fpath <- "scripts/robj/coef-error/"
  fname <- paste0("dgn-", design, "-", tolower(method))
  obj_name <- gsub("-", "_", gsub("dgn-", "dgn", fname))
  full_path <- paste0(fpath, fname, ".RData")
  local({
    load(full_path)
    assign(obj_name, out, envir = parent.env(environment()))
  })
  return(get(obj_name))
}
```
```{r}
dgn9_pls2 <- load_local(9, method = "PLS2")
dgn9_senv <- load_local(9, method = "Senv")
```
```{r coef-plot, fig.asp=0.9, fig.width=8, out.width='100%', fig.cap="Regression Coefficients estimated by PLS2 and Simultaneous methods on the data based on Design 9."}
resp_lab <- function(x) paste0("Y", x)
comp_lab <- function(x) paste0("Comp:", x)
coef_plt_pls2 <- coef_plot(
  dgn9_pls2, ncomp = 10,
  labeller = labeller(.rows = resp_lab, .cols = comp_lab)
) +
  labs(y = "Coefficient (PLS2)",
       x = NULL, 
       title = "Regression Coefficients (Design: 9)", 
       subtitle = "Averaged over 50 Replicates") +
  theme(legend.position = "none")
coef_plt_senv <- coef_plot(
  dgn9_senv, ncomp = 10,
  labeller = labeller(.rows = resp_lab, .cols = comp_lab)
) +
  labs(y = "Coefficient (Senv)",
       x = "Predictors", title = NULL, subtitle = NULL)
gridExtra::grid.arrange(
  coef_plt_pls2,
  coef_plt_senv,
  ncol = 1
)
```

Figure \@ref(fig:err-plot) shows a clear distinction between the modelling approach of PLS2 and Senv methods for the same model based on Design 9. In the case of PLS2, both minimum prediction error and minimum estimation error are obtained using seven to eight components and the estimated regression coefficients approximate the true coefficients. In contrast, the Senv method has approached the minimum prediction and minimum estimation error using one to two components and the corresponding estimated regression coefficients approximate the true coefficients (Figure \@ref(fig:coef-plot)). Despite having contrast modelling result for a dataset with similar properties, the minimum errors produced by them are comparable (See Table \@ref(tab:min-err-dgn9)).

```{r}
Error_df <- bind_rows(
  PLS2 = left_join(
    map_df(dgn9_pls2, "prediction_error", .id = "Replication"),
    map_df(dgn9_pls2, "estimation_error", .id = "Replication"),
    by = c("Replication", "Tuning_Param", "Response")
  ),
  Senv = left_join(
    map_df(dgn9_senv, "prediction_error", .id = "Replication"),
    map_df(dgn9_senv, "estimation_error", .id = "Replication"),
    by = c("Replication", "Tuning_Param", "Response")
  ), .id = "Method") %>% 
  gather(Error_Type, Error, Pred_Error, Est_Error) %>% 
  mutate(Error_Type = case_when(
    Error_Type == "Pred_Error" ~ "Prediction",
    TRUE ~ "Estimation"
  ))
```
```{r err-plot, fig.asp=0.8, fig.width=8, out.width='100%', fig.cap="Minimum prediction and estimation error for PLS2 and Simultaneous methods. The point and lines are averaged over 50 replications."}
Error_df %>% 
  filter(Error <= 500) %>% 
  mutate(Response = paste0("Y", Response),
         Tuning_Param = factor(Tuning_Param, levels = 0:10)) %>% 
  ggplot(aes(
    x = Tuning_Param,
    y = Error,
    fill = Response)) + 
  stat_summary(fun.y = "mean", geom = "line", 
               position = position_dodge(width = 0.8),
               aes(color = Response, group = Response)) +
  stat_summary(fun.y = "mean", geom = "point", 
               size = 0.5, shape = 21,
               position = position_dodge(width = 0.8)) +
  geom_boxplot(alpha = 0.3, outlier.colour = "grey60",
               color = "grey 40", size = 0.15) +
  facet_grid(Error_Type ~ Method, scales = 'free') +
  scale_fill_brewer(palette = "Set1") +
  scale_color_brewer(palette = "Set1") +
  labs(x = "Number of Components",
       y = "Error Value",
       title = "Prediction and Estimation Error",
       subtitle = "Design: 9, Averaged over 50 replicates")+
  theme(legend.position = "bottom")
```

The Figure \@ref(fig:err-plot) also shows that Senv has resulted in huge estimation error when the number of components is not optimal. This is also true for the PLS2 model however the extent of this variation is noticeably large in the Senv method. A similar observation as Senv is also found in Xenv method while PCR and PLS1 are closer to the PLS2 in terms of their use of components in order to produce the minimum error (See Table \@ref(tab:min-err-dgn9)).

```{r}
min_err <- pred_error %>% 
  inner_join(est_error, by = names(est_error)[1:5]) %>%
  filter(Design == 9) %>% 
  rename(Prediction = Pred_Error,
         Estimation = Est_Error) %>% 
  gather(Error_Type, Error, Prediction, Estimation) %>% 
  group_by(Method, Response, Tuning_Param, Error_Type) %>% 
  dplyr::summarize(Error = mean(Error)) %>% 
  group_by(Method, Response, Error_Type) %>% 
  dplyr::summarize(Components = Tuning_Param[which.min(Error)],
            Error = min(Error),
            Error_ = paste0(round(Error, 2), " (", Components, ")")) %>% 
  select(-Error, -Components) %>% 
  spread(Method, Error_) %>% 
  ungroup() %>% 
  arrange(Error_Type, Response)
```
```{r min-err-dgn9}
min_err %>% 
  select(-Error_Type) %>% 
  knitr::kable(
    align = 'r', booktabs = TRUE,
    format = ifelse(knitr::is_latex_output(), "latex", "html"),
    caption = "Minimum Prediction and Estimation Error for Design 9") %>% 
  kableExtra::kable_styling(full_width = TRUE) %>%
  kableExtra::column_spec(4:5, italic = TRUE) %>% 
  kableExtra::group_rows("Estimation Error", 1, 4) %>% 
  kableExtra::group_rows("Prediction Error", 5, 8)
```

Despite having a large variation in prediction and estimation error, the envelope based methods have produced a better result even in the difficult model as obtained from Design 9.

# Analysis

## Dataset for Analysis
## Discuss Model

```{r manova-model}
est_mdl <- lm(
  formula = cbind(Y1, Y2, Y3, Y4) ~ (p + gamma + eta + relpos + Method) ^ 3,
  data = est_min)
comp_mdl <- lm(
  formula = cbind(Y1, Y2, Y3, Y4) ~ (p + gamma + eta + relpos + Method) ^ 3,
  data = comp_min)
```

```{r manova-summary}
est_aov <- anova(est_mdl) %>%
  as_tibble(rownames = "Factors")
comp_aov <- anova(comp_mdl) %>%
  as_tibble(rownames = "Factors")
aov_df <- bind_rows(list(Est = est_aov, Comp = comp_aov), .id = "Type")
```

(ref:manova-plot) Pillai Statistic and F-value for the MANOVA model. The bar represents the Pillai Statistic and the text labels are F-value for corresponding factor.

```{r manova-plot, fig.width=8, out.width='100%', fig.asp=0.8, fig.cap="(ref:manova-plot)"}
model_labels <- c(
  Comp = "Model: Number of Components",
  Est = "Model: Estimation Error"
)
aov_df %>%
    filter(!(Factors %in% c('Residuals', '(Intercept)'))) %>%
    select(Model = Type, Factors, Pillai,
           Fvalue = `approx F`, Pvalue = `Pr(>F)`) %>%
    mutate(Model = factor(Model, levels = c("Est", "Comp"))) %>%
    mutate(Pvalue = ifelse(Pvalue < 0.05, "<0.05", ">=0.05")) %>%
    ggplot(aes(reorder(Factors, Pillai), Pillai, fill = Pvalue)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = round(Fvalue, 2)), family = 'mono',
              angle = 0, hjust = "inward", size = 3) +
    facet_grid(cols = vars(Model), scales = 'free_y',
               labeller = labeller(Model = model_labels)) +
    theme_grey(base_family = "mono") +
    theme(legend.position = c(0.85, 0.1),
          legend.direction = 'horizontal',
          axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
    guides(fill = guide_legend(title.position = "top",
                               title.hjust = 0.5)) +
    labs(x = NULL, y = "Pillai Statistic") +
    coord_flip()
```

## Interpretation of the fitted MANOVA Model

## Effect of MANOVA

### Error Model


(ref:est-eff-plot) Effect plot of some interactions of the multivariate linear model of estimation error

```{r est-eff-plots, fig.width=7, out.width='100%', fig.cap='(ref:est-eff-plot)', fig.asp = 0.6}
thm <- theme(plot.title = element_blank(),
             plot.subtitle = element_blank(),
             legend.position = "top",
             axis.title = element_blank())
plt1 <- eff_df("eta:relpos:Method", est_mdl) %>%
  eff_plot3(reorder = FALSE, labeller = label_both,
            scales = "free_y") +
  theme_grey(base_family = "mono") +
  thm
plt2 <- eff_df("relpos:gamma:Method", est_mdl) %>%
  eff_plot3(reorder = FALSE, labeller = label_both,
            scales = "free_y") +
  theme_grey(base_family = "mono") +
  thm
plt <- gridExtra::arrangeGrob(plt1, plt2, ncol = 2,
                       bottom="Methods", padding = unit(0.04, 'npc'),
                       left = "Fitted Estimation Error")
grid::grid.newpage()
grid::grid.draw(plt)
```


- Effect analysis of estimation error model
- Tie up these results with prediction error in previous paper

### Component Model

(ref:comp-eff-plot) Effect plot of some interactions of the multivariate linear model of number of components to get minimum prediction error

```{r comp-eff-plots, fig.width=7, out.width='100%', fig.cap='(ref:comp-eff-plot)', fig.asp = 0.6, fig.pos="!htb"}
thm <- theme(plot.title = element_blank(),
             plot.subtitle = element_blank(),
             legend.position = "top",
             axis.title = element_blank())
plt1 <- eff_df("eta:relpos:Method", comp_mdl) %>%
  eff_plot3(reorder = FALSE, labeller = label_both) +
  theme_grey(base_family = "mono") +
  thm
plt2 <- eff_df("relpos:gamma:Method", comp_mdl) %>%
  eff_plot3(reorder = FALSE, labeller = label_both) +
  theme_grey(base_family = "mono") +
  thm
plt <- gridExtra::arrangeGrob(plt1, plt2, ncol = 2,
                       bottom="Methods", padding = unit(0.04, 'npc'),
                       left = "Fitted Number of Components")
grid::grid.newpage()
grid::grid.draw(plt)
```


- Effect analysis of number of component model
- Tie up these results in previous paper

# Discussion and Conclusion
- A similar discussion but based more on why the methods worked in the way we have seen in the results in previous sections
- Some concluding remarks and limitations (or a gate for further exploration)

`r if (knitr:::is_html_output()) '# References {-}'`
<!-- # References -->
