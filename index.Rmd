---
site: bookdown::bookdown_site
title: 'Comparison of Multivariate Estimation Methods'
date: "`r Sys.Date()`"
author:
  - name: Raju Rimal
    email: raju.rimal@nmbu.no
    affiliation: KBM
    footnote: Corresponding Author
  - name: Trygve Almøy
    email: trygve.almoy@nmbu.no
    affiliation: KBM
  - name: Solve Sæbø
    email: solve.sabo@nmbu.no
    affiliation: NMBU
address:
  - code: KBM
    address: Faculty of Chemistry and Bioinformatics, Norwegian University of Life Sciences, Ås, Norway
  - code: NMBU
    address: Prorector, Norwegian University of Life Sciences, Ås, Norway
classoption: ["12pt", "3p", "authoryear"]
monofont: 'sourcecodepro'
monofontoptions: "Scale=0.7"
colorlinks: true
linespacing: 1.5
fontfamily: mathpazo
use-landscape-page: yes
tables: yes
bibliography: 'ref-db.bib'
biblio-title: References
biblio-style: elsarticle-harv
link-citations: true
github-repo: therimalaya/04-estimation-comparison
always_allow_html: true
url: 'http\://therimalaya.github.io/04-estimation-comparison'
knit: 'bookdown::render_book'
review: yes
keywords: ['model-comparison ', 'multi-response ', 'simrel ', 'estimation ', 'estimation error ', 'meta modeling ']
abstract: |
  Prediction performance often does not reflect the estimation behaviour of a method. High error in estimation not necessarily results in high prediction error but can lead to an unreliable prediction when test data are in a different direction than the training data. In addition, the effect of a variable becomes unstable and can not be interpreted in such situations. Many research fields are more interested in these estimates than performing prediction. This study compares some newly-developed (envelope) and well-established (PLS, PCR) prediction methods using simulated data with specifically designed properties such as multicollinearity, the correlation between multiple responses and position of principal components of predictor that are relevant for the response. This study aims to give some insight into these methods and help the researcher to understand and use them for further study. _Write some specifics from the results to show what we have found._
editor_options: 
  chunk_output_type: console
---

```{r, echo = FALSE, warning=FALSE, message=FALSE, cache=FALSE}
options(digits = 3, scipen = 999)
source("scripts/00-function.r")
source("scripts/01-setup.r")
knitr::opts_chunk$set(comment = NULL, out.width = "100%", echo = FALSE)
if (knitr::is_html_output()) {
  knitr::opts_chunk$set(dev = 'svg', fig.retina = 2)
}
extrafont::loadfonts()
```

# Introduction #

Estimation of parameters in a regression model is an integral part of many research study. Research fields such as social science, econometrics, psychology and medical study are more interested in measuring the impact of certain indicator or variable rather than performing prediction. Such studies have a large influence on people’s perception and also help in policy making and decisions.

Technology has facilitated researcher to collect a large amount of data however often times, such data either contains irrelevant information or are highly collinear. Researchers are devising new estimators to extract information and identify their inter-relationship. Some estimators are robust towards fixing the multicollinearity problem while some are targeted to model only the relevant information contained in the response variable.

This study extends the [@rimal2019pred] and compares some well-established estimators such as Principal Components Analysis (PCA), Partial Least Squares (PLS) together with two new methods based on envelope estimation: Envelope estimation in predictor space (Xenv) [@cook2010envelope] and simultaneous estimation of envelope (Senv) [@cook2015simultaneous]. The estimation process of these methods is discussed in [Methods] section. The comparison tests the estimation performance of these methods using multi-response simulated data from a linear model with controlled properties. The properties include the number of predictors, level of multicollinearity, the correlation between different response variables and the position of relevant predictor components. These properties are explained in [Experimental Design] section together with the strategy behind the simulation and data model.

# Simulation Model #

* Reduction of the regression model
  + Include the figure from previous paper
* How the covariance and coefficients are related
* From the construction of the covariance matrix of latent variables to the simulated data
* How and what simulation parameters are related to properties of data

# Estimation Methods #

A regression model is written as,

\begin{equation}
\underset{(1\times m)}{\mathbf{y}} =
  \underset{(1\times p)(p\times m)}
    {\mathbf{x}\boldsymbol{\beta}} +
  \underset{(1 \times m)}{\boldsymbol{\varepsilon}}
(\#eq:reg-model)
\end{equation}
where $\mathbf{y}$ is a vector of $m$ responses measured about their means, $\mathbf{x}$ is a vector of $p$ predictors measured about their means, $\boldsymbol{\beta}$ is a matrix of regression coefficients and $\boldsymbol{\varepsilon}$ is a vector of independent error terms with constant variance $\boldsymbol{\Sigma}_{y|x}$. In ordinary least squares, coefficient $\boldsymbol{\beta}$ is estimated as,

\begin{equation}
\underset{(p\times m)}{\boldsymbol{\hat{\beta}}} =
  \left(\underset{(p\times n)}{\mathbf{x}^t}
  \underset{(n\times p)}{\mathbf{x}}\right)^{-1}
  \underset{(p \times n)(n \times m)}{\mathbf{x}^t\mathbf{y}}
(\#eq:reg-beta)
\end{equation}

## Principal Components Regression ##

Principal Components are new set of variables from the transformation of original dataset such that they are uncorrelated with each other and the variation in the original data are ordered from first to last of these new variables. Let us define a transformation of $\mathbf{x}$ as,

\begin{equation}
\underset{(1\times p)}{\mathbf{x}} =
  \underset{(1 \times k)}{\mathbf{z}}
  \underset{(k \times p)}{\mathbf{R}^t}
(\#eq:x2z)
\end{equation}
where $\mathbf{R}$ is the eigenvectors corresponding to the covariance of
$\mathbf{x}$ and $\mathbf{z}$ are the principal components. A regression model  can be defined in terms of $\mathbf{z}$ as,

\begin{equation}
\underset{(1 \times m)}{\mathbf{y}} =
  \underset{(1 \times k)}{\mathbf{z}}
    \underset{(k \times m)}{\boldsymbol{\alpha}} +
  \underset{(1 \times m)}{\boldsymbol{\varepsilon}}
(\#eq:latent-model)
\end{equation}

Since the variation is ordered in $z$, only $k\le p$ columns of $z$ are used so that $p-k$ uninformative components are not used for modeling. The regression coefficient of \@ref(eq:latent-model) can be estimated as,

\begin{equation}
\underset{(k\times m)}{\boldsymbol{\hat{\alpha}}} =
  \left(\underset{(k \times n)}{\mathbf{z}^t}
    \underset{(n \times k)}{\mathbf{z}}\right)^{-1}
  \underset{(k \times n)(n \times m)}{\mathbf{z}^t\mathbf{y}}
(\#eq:reg-alpha)
\end{equation}

Using \@ref(eq:x2z) in \@ref(eq:reg-beta), we get,

$$
\begin{aligned}
\underset{(p\times m)}{\boldsymbol{\hat{\beta}}} =
  \left[
    \underset{(p\times k)(k \times n)}{\mathbf{R}\mathbf{z}^t}
    \underset{(n\times k)(k \times p)}{\mathbf{z} \mathbf{R}^t}
  \right]^{-1}
  \underset{(p\times k)(k \times n)}{\mathbf{R}\mathbf{z}^t}
  \underset{(n\times m)}{y}
\end{aligned}
$$

## Partial Least Squares Regression ##
- How beta coefficients are constructed
- How it is dependent on the variance-covariance matrices
- In what way PLS1 and PLS2 differ

## Envelope Estimations ##
- How beta coefficients are constructed
- How it is dependent on the variance-covariance matrices
- In what way Xenv and Senv differ

# Experimental Design #
An R [@coreR2018] package `simrel` [@Rimal2018; @saebo2015simrel] is used to simulate data. For the simulation the number of observation is kept fixed at $n = 100$ and following four simulation parameters are used to obtain the data with wide range of properties.

**Number of predictors:**
: In order to cover both tall $(n>p)$ and wide $(p>n)$ cases, $p=20$ and $p=250$ number of predictors are simulated.

**Multicollinearity in predictor variables:**
: A parameter `gamma` $(\gamma)$ in simulation controls the exponential decline of eigenvalues $(\lambda_i, i = 1, \ldots p)$ corresponding to predictor variables as,
  \begin{equation}
  \lambda_i = e^{-\gamma(i-1)}, \gamma > 0 \text{ and } i = 1, 2, \ldots p
  (\#eq:gamma)
  \end{equation}
: Two levels 0.2 and 0.8 of `gamma` are used for simulation so that level 0.2 simulates the data with low multicollinearity and 0.8 simulates the data with high multicollinearity.
  
**Position of relevant components:**
: Initial principal components of a non-singular covariance matrix are larger than the later one. If the principal components corresponding to predictors with larger variation is not relevant for a response, this will just increase noise in the model. Here we will use two different levels of position index of predictor components: a) 1, 2, 3, 4 and b) 5, 6, 7, 8. Predictor components irrelevant for a response makes prediction difficult [@Helland1994b]. When combined with multicollinearity, this factor can create both easy and difficult model for both estimation and prediction.

```{r design-plot, fig.cap="Experimental Design of simulation parameters. Each point represents an unique data property.", echo = FALSE, fig.asp=0.4, fig.width=8}
design_chr %>%
    mutate(Design = row_number()) %>%
    ggplot(aes(eta, gamma)) +
    geom_point(shape=4) +
    ggrepel::geom_text_repel(
      aes(label = Design), 
      nudge_x = 0.03, family = 'mono', fontface = "bold") +
    facet_grid(p ~ relpos, labeller=label_both) +
    scale_y_reverse(breaks = opts$gamma) +
    scale_x_continuous(breaks = opts$eta) +
    theme_minimal(base_size = 16, base_family = "mono") +
    theme(panel.grid.minor = element_blank(), text = element_text(face = "bold")) +
    coord_fixed(ratio=0.5)
```

- Discuss the design
  + Include the design plot from previous paper
- What nature of data properties is captured by the design and how diverse is it
- What is the limitation
  + Discuss about the single latent component of response space

## Basis of Comparison
- Similar to previous paper but for estimation error
- Here we will also compare the estimated and true regression coefficient for each additional component
  + We can do this through graph
  + Is there some way we can do this through statistics

# Exploration #
- A similar exploration as previous paper but can be different in case of new idea

## Dataset for Analysis ##
- Preparation of dataset for MANOVA analysis (i.e. minimum estimation error using arbitrary number of components)
- A component dataset is also created for testing the use of components by each of these methods

## Regression Coefficients ##
- In case of some idea on comparing regression coefficients through some statistical way, this can be included here
- Otherwise can also be done just by using plots

## Prediction and Estimation Error ##
- Explore both estimation error and number of components and try to bind them with the prediction error for the similar case

# Analysis #
- A MANOVA model is fitted using the dataset prepared in previous section

## Error Analysis ##
- Effect analysis of estimation error model
- Tie up these results with prediction error in previous paper

## Component Analysis ##
- Effect analysis of number of component model
- Tie up these results in previous paper

# Discussion and Conclusion #
- A similar discussion but based more on why the methods worked in the way we have seen in the results in previous sections
- Some concluding remarks and limitations (or a gate for further exploration)

`r if (knitr:::is_html_output()) '# References {-}'`


























































































