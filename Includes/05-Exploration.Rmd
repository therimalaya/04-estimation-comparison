# Exploration

This section explores the variation in the _error dataset_ and the _component dataset_ for which we have used Principal Component Analysis (PCA). Let $t_u$ and $t_v$ be the principal component score sets corresponding to PCA run on the $\mathbf{u}$ and $\mathbf{v}$ matrices respectively. The scores density in Figure \@ref(fig:est-pca-hist-mthd-gamma-relpos) and Figure \@ref(fig:comp-pca-hist-mthd-gamma-relpos) correspond to the first principal component of $\mathbf{u}$ and $\mathbf{v}$, i.e. the first column of $t_u$ and $t_v$ respectively.

```{r pca}
est_pca <- with(est_min, prcomp(cbind(Y1, Y2, Y3, Y4)))
expl_var <- explvar(est_pca) %>% round(2)
est_dta_with_pc <- bind_cols(est_min, as.data.frame(scores(est_pca)[]))
comp_pca <- with(comp_min, prcomp(cbind(Y1, Y2, Y3, Y4)))
comp_expl_var <- explvar(comp_pca) %>% round(2)
comp_dta_with_pc <- bind_cols(comp_min, as.data.frame(scores(comp_pca)[]))
```

```{r pc-hist-plot-function}
pc_density_plot <- function(dta, expl_var, title) {
    dta %>%
        ggplot(aes(PC1, eta, fill = relpos)) +
        geom_density_ridges(
            scale = 1,
            alpha = 0.4, size = 0.25) +
        geom_density_ridges(
            scale = 0.95,
            alpha = 0.2, size = 0.25,
            stat = "binline", bins = 30) +
        facet_wrap(
            . ~ interaction(Method, paste0("gamma:", gamma), sep = "|"),
            scales = 'free_x', ncol = 5,
            labeller = labeller(gamma = label_both, p = label_both)) +
        theme_grey(base_family = 'mono') +
        theme(
            legend.position = "bottom",
            strip.text = element_text(family = "mono")) +
        labs(x = paste0("PC1(", expl_var[1], "%)")) +
        ggtitle(title) +
        scale_x_continuous(breaks = scales::pretty_breaks(3)) +
        scale_color_brewer(palette = "Set1") +
        scale_fill_brewer(palette = "Set1")
}
```

(ref:est-hist) Scores density corresponding to first principal component of _error dataset_ ($\mathbf{u}$) subdivided by `methods`, `gamma` and `eta` and grouped by `relpos`.

```{r est-pca-hist-mthd-gamma-relpos, message=FALSE, warning=FALSE, fig.cap="(ref:est-hist)", fig.pos="!htb"}
pc_density_plot(est_dta_with_pc, expl_var,
                title = "Density of PCA scores for error model")
```

The plot shows a clear difference between the effect of low and high multicollinearity in estimation error. In the case of low multicollinearity (`gamma: 0.2`), the estimation errors are smaller and have lesser variation compared to high multicollinearity (`gamma: 0.9`). High multicollinearity has a larger influence on all but noticeably in the methods based on envelopes. Some large estimation error in the envelope is more than 100 which in the case of other methods is less than 60.

Furthermore, the relevant predictor components, in general, has a noticeable effect on estimation error. When relevant predictors are at position 5, 6, 7, 8, the predictor components at 1, 2, 3, 4, which carry most of the variation, becomes irrelevant. These irrelevant components with large variation add noise to the model and consequently increases the estimation error. The effect intensifies on highly collinear predictors. Designs with high multicollinearity and relevant predictors at position 5, 6, 7, 8 are relatively difficult to model for all the methods. Although these difficult designs have a large effect on estimation error, their effect on prediction error is less influential [@rimal2019pred].

(ref:comp-hist) Score density corresponding to first principal component of _component dataset_ ($\mathbf{v}$) subdivided by `methods`, `gamma` and `eta` and grouped by `relpos`.

In the case of the _component dataset_ (Figure Above), PCR, PLS1 and PLS2 methods have used more components in the case of high multicollinearity compared to low. Surprisingly, the envelope methods (Senv and Xenv) mostly have used a distinctly lesser number of components in both the cases of multicollinearity compared to other methods.

The plot also shows that there is no clear effect due to the correlation of response variable on the number of components used to obtain minimum estimation error.

```{r comp-pca-hist-mthd-gamma-relpos, message=FALSE, warning=FALSE, fig.cap="(ref:comp-hist)", fig.pos="!htb"}
pc_density_plot(comp_dta_with_pc, comp_expl_var,
                title = "Density of PCA scores for component model")
```


A clear interaction between the position of relevant predictors and the multicollinearity visible in the plot suggest that the methods use a larger number of components when the relevant components are at position 5, 6, 7, 8. Additionally, the use of components escalate and the difference between the two levels of `relpos` becomes wider in the case of high multicollinearity in the model. Such performance is also seen the the case of prediction error (See @rimal2019pred) however the number of components used in that case is lesser than in this case. Envelope methods, however, have shown a distinct result in contrast to the other methods. Even when the relevant predictors are at position 5, 6, 7, 8, the envelope methods, in contrast to other methods, have used an almost similar number of components as in the case of relevant predictor at position 1, 2, 3, 4. This shows that the envelope methods identify the predictor space relevant to the response differently and with few numbers of latent components.

Following section explore the prediction and estimation error together with the regression coefficient of Simultaneous Envelope and Partial Least Squares for a design having high multicollinearity with predictor components at position 5, 6, 7, 8. Here we will use design with $n>p$ and no correlation between the response which corresponds to Design-9.

```{r}
load_local <- function(design, method) {
  fpath <- "scripts/robj/coef-error/"
  fname <- paste0("dgn-", design, "-", tolower(method))
  obj_name <- gsub("-", "_", gsub("dgn-", "dgn", fname))
  full_path <- paste0(fpath, fname, ".RData")
  local({
    load(full_path)
    assign(obj_name, out, envir = parent.env(environment()))
  })
  return(get(obj_name))
}
```
```{r}
dgn9_pls2 <- load_local(9, method = "PLS2")
dgn9_senv <- load_local(9, method = "Senv")
```
```{r coef-plot, fig.asp=0.9, fig.width=8, out.width='100%', fig.cap="Regression Coefficients estimated by PLS2 and Simultaneous methods on the data based on Design 9."}
resp_lab <- function(x) paste0("Y", x)
comp_lab <- function(x) paste0("Comp:", x)
coef_plt_pls2 <- coef_plot(
  dgn9_pls2, ncomp = 10,
  labeller = labeller(.rows = resp_lab, .cols = comp_lab)
) +
  labs(y = "Coefficient (PLS2)",
       x = NULL, 
       title = "Regression Coefficients (Design: 9)", 
       subtitle = "Averaged over 50 Replicates") +
  theme(legend.position = "none")
coef_plt_senv <- coef_plot(
  dgn9_senv, ncomp = 10,
  labeller = labeller(.rows = resp_lab, .cols = comp_lab)
) +
  labs(y = "Coefficient (Senv)",
       x = "Predictors", title = NULL, subtitle = NULL)
gridExtra::grid.arrange(
  coef_plt_pls2,
  coef_plt_senv,
  ncol = 1
)
```

Figure \@ref(fig:err-plot) shows a clear distinction between the modelling approach of PLS2 and Senv methods for the same model based on Design 9. In the case of PLS2, both minimum prediction error and minimum estimation error are obtained using seven to eight components and the estimated regression coefficients approximate the true coefficients. In contrast, the Senv method has approached the minimum prediction and minimum estimation error using one to two components and the corresponding estimated regression coefficients approximate the true coefficients (Figure \@ref(fig:coef-plot)). Despite having contrast modelling result for a dataset with similar properties, the minimum errors produced by them are comparable (See Table \@ref(tab:min-err-dgn9)).

```{r}
Error_df <- bind_rows(
  PLS2 = left_join(
    map_df(dgn9_pls2, "prediction_error", .id = "Replication"),
    map_df(dgn9_pls2, "estimation_error", .id = "Replication"),
    by = c("Replication", "Tuning_Param", "Response")
  ),
  Senv = left_join(
    map_df(dgn9_senv, "prediction_error", .id = "Replication"),
    map_df(dgn9_senv, "estimation_error", .id = "Replication"),
    by = c("Replication", "Tuning_Param", "Response")
  ), .id = "Method") %>% 
  gather(Error_Type, Error, Pred_Error, Est_Error) %>% 
  mutate(Error_Type = case_when(
    Error_Type == "Pred_Error" ~ "Prediction",
    TRUE ~ "Estimation"
  ))
```
```{r err-plot, fig.asp=0.8, fig.width=8, out.width='100%', fig.cap="Minimum prediction and estimation error for PLS2 and Simultaneous methods. The point and lines are averaged over 50 replications."}
Error_df %>% 
  filter(Error <= 500) %>% 
  mutate(Response = paste0("Y", Response),
         Tuning_Param = factor(Tuning_Param, levels = 0:10)) %>% 
  ggplot(aes(
    x = Tuning_Param,
    y = Error,
    fill = Response)) + 
  stat_summary(fun.y = "mean", geom = "line", 
               position = position_dodge(width = 0.8),
               aes(color = Response, group = Response)) +
  stat_summary(fun.y = "mean", geom = "point", 
               size = 0.5, shape = 21,
               position = position_dodge(width = 0.8)) +
  geom_boxplot(alpha = 0.3, outlier.colour = "grey60",
               color = "grey 40", size = 0.15) +
  facet_grid(Error_Type ~ Method, scales = 'free') +
  scale_fill_brewer(palette = "Set1") +
  scale_color_brewer(palette = "Set1") +
  labs(x = "Number of Components",
       y = "Error Value",
       title = "Prediction and Estimation Error",
       subtitle = "Design: 9, Averaged over 50 replicates")+
  theme(legend.position = "bottom")
```

The Figure \@ref(fig:err-plot) also shows that Senv has resulted in huge estimation error when the number of components is not optimal. This is also true for the PLS2 model however the extent of this variation is noticeably large in the Senv method. A similar observation as Senv is also found in Xenv method while PCR and PLS1 are closer to the PLS2 in terms of their use of components in order to produce the minimum error (See Table \@ref(tab:min-err-dgn9)).

```{r}
min_err <- pred_error %>% 
  inner_join(est_error, by = names(est_error)[1:5]) %>%
  filter(Design == 9) %>% 
  rename(Prediction = Pred_Error,
         Estimation = Est_Error) %>% 
  gather(Error_Type, Error, Prediction, Estimation) %>% 
  group_by(Method, Response, Tuning_Param, Error_Type) %>% 
  dplyr::summarize(Error = mean(Error)) %>% 
  group_by(Method, Response, Error_Type) %>% 
  dplyr::summarize(Components = Tuning_Param[which.min(Error)],
            Error = min(Error),
            Error_ = paste0(round(Error, 2), " (", Components, ")")) %>% 
  select(-Error, -Components) %>% 
  spread(Method, Error_) %>% 
  ungroup() %>% 
  arrange(Error_Type, Response)
```
```{r min-err-dgn9}
min_err %>% 
  select(-Error_Type) %>% 
  knitr::kable(
    align = 'r', booktabs = TRUE,
    format = ifelse(knitr::is_latex_output(), "latex", "html"),
    caption = "Minimum Prediction and Estimation Error for Design 9") %>% 
  kableExtra::kable_styling(full_width = TRUE) %>%
  kableExtra::column_spec(4:5, italic = TRUE) %>% 
  kableExtra::group_rows("Estimation Error", 1, 4) %>% 
  kableExtra::group_rows("Prediction Error", 5, 8)
```

Despite having a large variation in prediction and estimation error, the envelope based methods have produced a better result even in the difficult model as obtained from Design 9.

