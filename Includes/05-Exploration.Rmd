---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Exploration

In this section we explore the variation in the _error dataset_ and the _component dataset_ by means of Principal Component Analysis (PCA). Let $\mathbf{t}_u$ and $\mathbf{t}_v$ be matrices holding the column vectors of the principal component scores corresponding to the $\mathbf{u}$ and $\mathbf{v}$ matrices, respectively. The density of the scores in Figure \@ref(fig:est-pca-hist-mthd-gamma-relpos) and Figure \@ref(fig:comp-pca-hist-mthd-gamma-relpos) correspond to the first principal component of $\mathbf{u}$ and $\mathbf{v}$, i.e. the first column of $\mathbf{t}_u$ and $\mathbf{t}_v$ respectively. Here higher scores correspond to larger estimation error and vice versa.

```{r pca}
est_pca <- with(est_min, prcomp(cbind(Y1, Y2, Y3, Y4)))
expl_var <- explvar(est_pca) %>% round(2)
est_dta_with_pc <- bind_cols(est_min, as.data.frame(scores(est_pca)[]))
comp_pca <- with(comp_min, prcomp(cbind(Y1, Y2, Y3, Y4)))
comp_expl_var <- explvar(comp_pca) %>% round(2)
comp_dta_with_pc <- bind_cols(comp_min, as.data.frame(scores(comp_pca)[]))
```

```{r pc-hist-plot-function}
pc_density_plot <- function(dta, expl_var, title, facet_vars = c("Method", "gamma"),
                            x_var = PC1, y_var = eta, fill_var = relpos, ncol = 5) {
  x_var <- enquo(x_var)
  y_var <- enquo(y_var)
  fill_var <- enquo(fill_var)
  plt <- dta %>%
    ggplot(aes(!!x_var, !!y_var, fill = !!fill_var)) +
    geom_density_ridges(
      scale = 1,
      alpha = 0.4, size = 0.25) +
    geom_density_ridges(
      scale = 0.95,
      alpha = 0.2, size = 0.25,
      stat = "binline", bins = 30) +
    facet_grid(as.formula(paste(facet_vars, collapse = "~")),
               scales = 'free_x', labeller = label_both) +
    theme(
      legend.position = "bottom",
      strip.text = element_text(family = "mono")) +
    labs(x = paste0("PC1(", expl_var[1], "%)")) +
    ggtitle(title) +
    scale_x_continuous(breaks = scales::pretty_breaks(3)) +
    scale_color_brewer(palette = "Set1") +
    scale_fill_brewer(palette = "Set1")
  return(plt)
}
```

(ref:est-hist) Scores density corresponding to first principal component of _error dataset_ ($\mathbf{u}$) subdivided by `methods`, `gamma` and `eta` and grouped by `relpos`.

```{r est-pca-hist-mthd-gamma-relpos, message=FALSE, warning=FALSE, fig.cap="(ref:est-hist)", fig.pos="!htb", fig.asp=0.8}
plt <- est_dta_with_pc %>%
    filter(PC1 <= 40) %>%
    pc_density_plot(
      expl_var = expl_var,
      x_var = PC1,
      y_var = Method,
      fill_var = relpos,
      ncol = 4,
      facet_vars = c("eta", "gamma + p"),
      title = "Density of PCA scores corresponding to error dataset")
suppressMessages({print(plt)})
```

Figure \@ref(fig:est-pca-hist-mthd-gamma-relpos) shows a clear difference in the effect of low and high multicollinearity on estimation error. In the case of low multicollinearity (`gamma: 0.2`), the estimation errors are in general smaller and have lesser variation compared to high multicollinearity (`gamma: 0.9`). In particular we observe that the envelope methods have small estimation errors in the low multicollinearity cases compared to the other methods. 

Furthermore, position of the relevant predictor components has a noticeable effect on estimation error for all methods. When relevant predictors are at position 5, 6, 7, 8, the components at positions 1, 2, 3, 4, which carry most of the variation, become irrelevant. These irrelevant components with large variation add noise to the model and consequently increases the estimation error. The effect intensifies with highly collinear predictors (`gamma`=0.9). Designs with high multicollinearity and relevant predictors at position 5, 6, 7, 8 are relatively difficult to model for all the methods. Although these difficult designs have a large effect on estimation error, their effect on prediction error is less influential [@rimal2019pred].

(ref:comp-hist) Score density corresponding to the first principal component of _component dataset_ ($\mathbf{v}$) subdivided by `methods`, `gamma` and `eta` and grouped by `relpos`.

In the case of the _component dataset_ (Figure \@ref(fig:comp-pca-hist-mthd-gamma-relpos)), PCR, PLS1 and PLS2 methods have in general used a larger number of components in the case of high multicollinearity compared to low. Surprisingly, the envelope methods (Senv and Xenv) have mostly used a distinctly smaller number of components in both cases of multicollinearity compared to other methods.

The plot also shows that there is no clear effect of the correlation between response variables (`eta`) on the number of components used to obtain minimum estimation error.

```{r comp-pca-hist-mthd-gamma-relpos, message=FALSE, warning=FALSE, fig.cap="(ref:comp-hist)", fig.pos="!htb", fig.asp=0.8}
plt <- comp_dta_with_pc %>%
  pc_density_plot(
    expl_var = comp_expl_var,
    x_var = PC1,
    y_var = Method,
    fill_var = relpos,
    ncol = 4,
    facet_vars = c("eta", "gamma + p"),
    title = "Density of PCA scores corresponding to component dataset")
suppressMessages({print(plt)})
```

A clear interaction between the position of relevant predictors and the multicollinearity, which is visible in the plot, suggests that the methods use a larger number of components when the relevant components are at position 5, 6, 7, 8. Additionally, the use of components escalate and the difference between the two levels of `relpos` becomes wider in the case of high multicollinearity in the predictor variables. Such performance is also seen the case of prediction error (See @rimal2019pred), however, the number of components used for optimization of prediction is smaller than in the case of estimation. Even when the relevant components are at position 5, 6, 7, 8, the envelope methods, in contrast to other methods, have used an almost similar number of components as in the case of relevant components at position 1, 2, 3, 4. This shows that the envelope methods identify the predictor space relevant to the response differently, from the other methods and with very few numbers of latent components. This is particularly the case when multicollinearity in $\mathbf{x}$ is high.

The following sub-section explores in particular the prediction and estimation errors and the estimated regression coefficient of Simultaneous Envelope and Partial Least Squares for a design having high multicollinearity, and with predictor components at positions 5, 6, 7, 8. Here we will use the design with $n>p$ and two levels of correlation between the responses. These correspond to Design-9 and Design-29 in our simulations.

```{r}
load_local <- function(design, method) {
  load_obj <- function(full_path, obj_name) {
    local({
      load(full_path)
      assign(obj_name, out, envir = parent.env(environment()))
    })
  }
  fpath <- "scripts/robj/coef-error/"
  crossing(design, method) %>%
    mutate(fname = paste0("dgn-", design, "-", tolower(method)),
           oname = gsub("-", "_", fname),
           full_path = paste0(fpath, fname, ".Rdata"),
           coef_obj = map2(full_path, oname, load_obj)) %>%
    select(-fname, -oname, -full_path)
}
```
```{r}
err_coef <- load_local(c(9, 29), c("PLS2", "Senv"))
```
```{r}
cf_plt <- err_coef %>%
  mutate(ncomp = ifelse(method == "PLS2", 10, 4)) %>%
  mutate(plot = pmap(list(design, method, coef_obj, ncomp), function(dgn, mthd, obj, ncomp) {
    resp_lab <- function(x) paste0("Y", x)
    comp_lab <- function(x) paste0("Comp:", x)
    atrb <- attributes(obj[[1]])[['Sim_Properties']]
    parms <- c("gamma", "relpos", "p", "eta")
    parms_chr <- sapply(atrb[parms], list2chr)
    st_lbl <- paste(names(parms_chr), parms_chr, sep = ":", collapse = ", ")
    coef_plot(obj, ncomp = ncomp, err_type = "estimation",
              labeller = labeller(.rows = resp_lab, .cols = comp_lab)) +
      labs(y = paste0("coef (", mthd, ")"),
           x = if(mthd == "Senv") "Components" else NULL,
           title = if(mthd == "PLS2") paste0("Regression Coefficients (Design: ", dgn, ")") else NULL,
           subtitle = if(mthd == "PLS2") st_lbl else NULL) +
      theme(legend.position = if(mthd == "PLS2") "none" else "bottom") +
      coord_cartesian(ylim = c(-0.8, 0.8))
  }))
```

```{r coef-plot, fig.asp=1.2, fig.width=10, fig.retina=2, out.width='100%', fig.cap="Regression Coefficients (coef) estimated by PLS2 and Simultaneous Envelope methods on the data based on Design 9 and 29."}
do.call(gridExtra::grid.arrange, 
        cf_plt %>% 
          pluck("plot") %>% 
          append(list(ncol = 1)))
```


Figure \@ref(fig:err-plot) shows a clear distinction between the modelling approach of PLS2 and Senv methods for the same model based on Design 9 (top) and Design 29 (bottom). In both of the designs, PLS2 has both minimum prediction error and minimum estimation error obtained using seven to eight components and the estimated regression coefficients approximate the true coefficients. In contrast, the Senv method has approached the minimum prediction and minimum estimation error using one to two components and the corresponding estimated regression coefficients approximate the true coefficients (Figure \@ref(fig:coef-plot)). Despite having contrasted modelling results for a dataset with similar properties, the minimum errors produced by them are comparable in the case of Design 9 (See Table \@ref(tab:min-err-dgn9)). However, in the case of Design 29, estimation error corresponding to PLS1 and envelope methods are much higher than PCR and PLS2. It is interesting to see that despite having large estimation error, the prediction error corresponding to the envelope methods are much smaller in this design.

In this study, the response dimension for the simultaneous envelope has been fixed at two components, which might have affected its performance, however, both envelope methods had performed much better with the same restriction in the case of prediction.

<!-- --- --- --- --- --- -->

```{r err-plot-df-9, message=FALSE, warning=FALSE}
err_df <- err_coef %>%
    mutate(prediction = map(coef_obj, map_df, "prediction_error",
                            .id = "Replication"),
           estimation = map(coef_obj, map_df, "estimation_error",
                            .id = "Replication")) %>%
    select(-coef_obj)
Error_df <- err_df %>% unnest_legacy(prediction) %>%
  inner_join(err_df %>% unnest_legacy(estimation),
             by = c("design", "method", "Replication", "Tuning_Param", "Response")) %>%
  gather(Error_Type, Error, Pred_Error, Est_Error) %>%
  rename(Method = method) %>%
  mutate(Error_Type = case_when(
    Error_Type == "Pred_Error" ~ "Prediction",
    TRUE ~ "Estimation"
  ))
plot_error <- function(error_df) {
  error_df %>%
  filter(Error <= 500) %>%
  mutate(Response = paste0("Y", Response),
         Tuning_Param = factor(Tuning_Param, levels = 0:10)) %>%
  ggplot(aes(
    x = Tuning_Param,
    y = Error,
    fill = Response)) +
  stat_summary(fun = "mean", geom = "line",
               position = position_dodge(width = 0.8),
               aes(color = Response, group = Response)) +
  stat_summary(fun = "mean", geom = "point",
               size = 0.5, shape = 21,
               position = position_dodge(width = 0.8)) +
  geom_boxplot(alpha = 0.3, outlier.colour = "grey60",
               color = "grey 40", size = 0.15) +
  facet_grid(Error_Type ~ Method, scales = 'free') +
  scale_fill_brewer(palette = "Set1") +
  scale_color_brewer(palette = "Set1") +
  labs(x = "Number of Components",
       y = "Error Value",
       title = "Prediction and Estimation Error",
       subtitle = paste0("Design: ", unique(error_df$design),
                         ", Averaged over 50 replicates"))+
  theme(legend.position = "bottom")
}
```

```{r err-plot, fig.asp=1.2, fig.width=9, out.width='100%', fig.cap="Minimum prediction and estimation error for PLS2 and Simultaneous Envelope methods. The point and lines are averaged over 50 replications."}
plt1 <- plot_error(Error_df %>% filter(design == 9))
plt2 <- plot_error(Error_df %>% filter(design == 29))
gridExtra::grid.arrange(plt1, plt2, ncol = 1)
```

Figure \@ref(fig:err-plot) also shows in both designs that Senv has large estimation errors when the number of components is not optimal. This is also true for the PLS2 model, however, the extent of this variation is noticeably large for the Senv method. A similar observation as Senv is also found in Xenv method while PCR and PLS1 are closer to the PLS2 in terms of their use of components in order to produce the minimum error (See Table \@ref(tab:min-err-dgn9)). Here, the variation in the estimation error can increase drastically also for PCR and PLS methods, when number of components more than 10 (not seen in the figure) is included. This is hinted in the estimation error plot (Figure \@ref(fig:err-plot)) for PLS2 for 8-10 number of components are included in the model.

In addition to the prediction and estimation error, Figure \@ref(fig:coef-plot) gives a closer view of how the average coefficients corresponding to these methods approximate to the true values. In the figure, PLS2 has used seven to eight components to reach the closest approximation to the true coefficients, but with increasing errors after including more components than eight. This departure from true coefficients is usual for PLS when the relevant components are at 1, 2, 3, 4 whereas PCR has shown more stable result in such situations. Further, the envelope methods have presented their ability to converge estimates to the true value in just one or two components. However, one should be cautious about determining the optimal number of components using method like cross-validation while working with real data.

```{r}
min_err <- pred_error %>%
  inner_join(est_error, by = names(est_error)[1:5]) %>%
  filter(Design %in% c(9, 29)) %>%
  rename(Prediction = Pred_Error,
         Estimation = Est_Error) %>%
  gather(Error_Type, Error, Prediction, Estimation) %>%
  group_by(Design, Method, Response, Tuning_Param, Error_Type) %>%
  dplyr::summarize(Error = mean(Error)) %>%
  group_by(Design, Method, Response, Error_Type) %>%
  dplyr::summarize(Components = Tuning_Param[which.min(Error)],
            Error = min(Error),
            Error_ = paste0(round(Error, 2), " (", Components, ")")) %>%
  select(-Error, -Components) %>%
  spread(Method, Error_) %>%
  ungroup() %>%
  arrange(desc(Design), Error_Type, Response)
```
```{r min-err-dgn9}
min_err %>%
  select(-Error_Type) %>%
  knitr::kable(
    align = 'r', booktabs = TRUE,
    format = ifelse(knitr::is_latex_output(), "latex", "html"),
    caption = "Minimum Prediction and Estimation Error for Design 9") %>%
  kableExtra::kable_styling(full_width = TRUE) %>%
  kableExtra::column_spec(5:6, bold = TRUE, italic = TRUE) %>%
  kableExtra::group_rows("Design 9", 1, 8) %>%
  kableExtra::group_rows("Design 29", 9, 16) %>%
  kableExtra::group_rows("Estimation Error", 1, 4) %>%
  kableExtra::group_rows("Prediction Error", 5, 8) %>%
  kableExtra::group_rows("Estimation Error", 9, 12) %>%
  kableExtra::group_rows("Prediction Error", 13, 16)
```

Despite having a large variation in prediction and estimation error, the envelope based methods have produced a better result even for the difficult data cases as shown for Design 9.
