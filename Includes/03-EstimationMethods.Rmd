---
bibliography: 'references.bib'
---

# Estimation Methods #

Consider a joint distribution of $\mathbf{y}$ and $\mathbf{x}$ with corresponding mean vectors $\boldsymbol{\mu}_y$ and $\boldsymbol{\mu}_x$ as,

\begin{equation}
  \begin{bmatrix}
    \mathbf{y} \\ \mathbf{x}
  \end{bmatrix} 
  \sim \mathsf{N}
  \begin{pmatrix}
    \begin{bmatrix}
      \boldsymbol{\mu}_y \\
      \boldsymbol{\mu}_x \\
    \end{bmatrix}, &&
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{yy} & \boldsymbol{\Sigma}_{yx} \\
      \boldsymbol{\Sigma}_{xy} & \boldsymbol{\Sigma}_{xx} 
    \end{bmatrix}
  \end{pmatrix}
  (\#eq:model-1)
\end{equation}

Here, $\boldsymbol{\Sigma}_{xx}$ and $\boldsymbol{\Sigma}_{yy}$ are variance-covariance of $\mathbf{x}$ and $\mathbf{y}$ respectively and $\boldsymbol{\Sigma}_{xy}=\boldsymbol{\Sigma}_{yx}^t$ is the covariance matrix of $\mathbf{x}$ and $\mathbf{y}$. Let $\mathbf{S}_{xx}$, $\mathbf{S}_{yy}$ and $\mathbf{S}_{xy}=\mathbf{S}_{yx}^t$ be the respective estimates of these matrices. A linear regression model based on \@ref(eq:model-1) is 

\begin{equation}
\mathbf{y} = \boldsymbol{\mu}_y + \boldsymbol{\beta}^t\left( \mathbf{x} - \boldsymbol{\mu}_{x} \right) + \boldsymbol{\varepsilon}
(\#eq:reg-model)
\end{equation}

where $\boldsymbol{\beta}=\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}$
is the regression coefficients that define the relationship between $\mathbf{x}$
and $\mathbf{y}$. With $n$ samples, the least-squares estimate of
$\boldsymbol{\beta}$ can be written as
$\boldsymbol{\hat{\beta}}=\mathbf{S}_{xx}^{-1}\mathbf{S}_{xy}$. Here, as in many
situations, the estimator $\mathbf{S}_{xx}$ for $\boldsymbol{\Sigma_{xx}}$ can either be non-invertible or have small eigenvalues. In addition, $\mathbf{S}_{xy}$, the estimator of $\boldsymbol{\Sigma_{xy}}$, is often influenced by a high level of noise in the data. In order to solve these problems, various methods have adopted the concept of relevant space to identify the relevant components through the reduction of the dimension in either $\mathbf{x}$ or $\mathbf{y}$ or both. Some of the methods we have used for comparison are discussed below.

_Principal Components Regression_ (PCR) uses $k$ eigenvectors of $\mathbf{S}_{xx}$ as the number of components to span the reduced relevant space. Since PCR is based on capturing the maximum variation in predictors for every component it has added to the model, this method does not consider the response structure in the model reduction [@Jolliffe2002]. In addition, if the relevant components are not corresponding to the largest eigenvalues, the method requires a larger number of components to make precise prediction [@Alm_y_1996].


_Partial Least Squares_ (PLS) regression aims to maximize the covariance between the predictor and response components (scores) [@DeJong1993]. Broadly speaking, PLS can be divided into PLS1 and PLS2 where the former tries to model the response variables individually, whereas the latter uses all the response variable together while modelling. Among the three widely used algorithms NIPALS [@wold75nipals], SIMPLS [@DeJong1993] and KernelPLS [@Lindgren_1993], we will be using KernelPLS for this study, which gives equivalent results to the classical NIPALS algorithm and is default in R-package `pls` [@mevik07_thepl].


_Envelopes_ was first introduced by [@Cook2007a] as the smallest subspace that includes the span of true regression coefficients. The _Predictor Envelope_ (Xenv) identifies the envelope as a smallest subspace in the predictor space, by separating the predictor covariance $\boldsymbol{\Sigma}_{xx}$ into relevant (material) and irrelevant (immaterial) parts, such that the response $\mathbf{y}$ is uncorrelated with the irrelevant part given the relevant one. In addition, relevant and irrelevant parts are also uncorrelated. Such separation of the covariance matrix is made using the data through the optimization of an objective function. Further, the regression coefficients are estimated using only the relevant part. @cook2010envelope, @cook2013envelopes and @cook2018envelope have extensively discussed the foundation and various mathematical constructs together with properties related to the Predictor Envelope. 

_Simultaneous Predictor-Response Envelope_ (Senv) implements the envelope in both the response and the predictor space. It separates the material and immaterial part in the response space and the predictor space such that the material part of the response does not correlate with the immaterial part of the predictor and the immaterial part of the response does not correlate with the material part of the predictor. The regression coefficients are computed using only the material part of the response and predictor spaces. The number of components specified in both of these methods during the fit influences the separation of these spaces. If the number of response components equals the number of responses, simultaneous envelope reduces to the predictor envelope, and if the number of predictor components equals the number of predictors, the result will be equivalent to ordinary least squares. @cook2015simultaneous and @cook2018envelope have discussed the method in detail. Further, @helland2016algorithms have discussed when and under which condition the population models of PCR, PLS and Xenv are equivalent.

Here, each methods uses different strategy for estimating the regression coefficients due to which the optimal number of components they determine will be different. For example, PCR method captures the maximum variation in predictor matrix in every subsequent components while PLS methods focus more on the variation in predictors that are relevant for the responses. The envelope methods construct the envelope as a linear combination of relevant eigenvectors. This allows them to reduce the dimension even further and consequently these methods need fewer components.
