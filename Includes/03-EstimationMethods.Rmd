---
bibliography: 'references.bib'
---

# Estimation Methods #

Consider a joint distribution of $\mathbf{y}$ and $\mathbf{x}$ with corresponding mean vectors $\boldsymbol{\mu}_y$ and $\boldsymbol{\mu}_x$ as,

\begin{equation}
  \begin{bmatrix}
    \mathbf{y} \\ \mathbf{x}
  \end{bmatrix} 
  \sim \mathsf{N}
  \begin{pmatrix}
    \begin{bmatrix}
      \boldsymbol{\mu}_y \\
      \boldsymbol{\mu}_x \\
    \end{bmatrix}, &&
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{yy} & \boldsymbol{\Sigma}_{yx} \\
      \boldsymbol{\Sigma}_{xy} & \boldsymbol{\Sigma}_{xx} 
    \end{bmatrix}
  \end{pmatrix}
  (\#eq:model-1)
\end{equation}

Here, $\boldsymbol{\Sigma}_{xx}$ and $\boldsymbol{\Sigma}_{yy}$ are variance-covariance of $\mathbf{x}$ and $\mathbf{y}$ respectively and $\boldsymbol{\Sigma}_{xy}=\boldsymbol{\Sigma}_{yx}^t$ is the covariance matrix of $\mathbf{x}$ and $\mathbf{y}$. Let $\mathbf{S}_{xx}$, $\mathbf{S}_{yy}$ and $\mathbf{S}_{xy}=\mathbf{S}_{yx}^t$ be the respective estimates of these matrices. A linear regression model based on \@ref(eq:model-1) is 

\begin{equation}
\mathbf{y} = \boldsymbol{\mu}_y + \boldsymbol{\beta}^t\left( \mathbf{x} - \boldsymbol{\mu}_{x} \right) + \boldsymbol{\varepsilon}
(\#eq:reg-model)
\end{equation}

where $\boldsymbol{\beta}=\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}$
is the regression coefficient that defines the relationship between $\mathbf{x}$
and $\mathbf{y}$. With $n$ samples, the least squares estimate of $\boldsymbol{\beta}$ can be written as $\boldsymbol{\hat{\beta}}=\mathbf{S}_{xx}^{-1}\mathbf{S}_{xy}$. Here in many situations $\mathbf{S}_{xx}$ can either non-invertible or has small eigenvalues. In addition, $\mathbf{S}_{xy}$ is influenced my high level of noise in the data. In order to solve these problems, various methods uses the concept of relevant space to identify the relevant components by reducing the dimension of either $\mathbf{x}$ or $\mathbf{y}$ or both.

_Principal Components Regression_ (PCR) uses $k$ eigenvectors of $\mathbf{S}_{xx}$ as the number of components or the number of reduced dimension. Since PCR is based on capturing the maximum variation in predictors on every component it has used to model, this method does not consider the response structure [@Jolliffe2002]. In addition, if the relevant components are not in the initial position, the method requires more number of components to make precise prediction [@Alm_y_1996].


_Partial Least Squares_ (PLS) regression tries to maximize the covariance between the predictors and the response scores [@DeJong1993]. Broadly, PLS can be divided into PLS1 and PLS2 where the former tries to model the response variables individually and the later using all the response variable together while modelling. Among the three widely used algorithms NIPALS [@wold75nipals], SIMPLS [@DeJong1993] and KernelPLS [@Lindgren_1993], for this study we will be using KernelPLS which gives equivalent results to the other two and is default in R-package `pls` [@mevik07_thepl].


_Envelopes_ is first introduced by [@Cook2007a] as the smallest subspace that includes the span of true regression coefficients. _Predictor Envelopes_ (Xenv) identifies the envelope as a smallest subspace in predictor space by separating the predictor covariance $\boldsymbol{\Sigma}_{xx}$ into relevant (material) and irrelevant (immaterial) parts such that response $\mathbf{y}$ is uncorrelated with the irrelevant part given the relevant one. In addition, relevant and irrelevant parts are also uncorrelated. Such separation of the covariance matrix is made using the data through optimization of the objective function. Further, the regression coefficients are estimated only using the relevant part. @cook2010envelope, @cook2013envelopes and @cook2018envelope have extensively discussed the foundation and various mathematical constructs together with properties related to Predictor Envelope. 

_Simultaneous Predictor-Response Envelope_ (Senv) implement the envelope in both response and predictor spaces. It separates the material and immaterial part in response space and predictor space such that the material part of response does not correlate with the immaterial part of predictor and the immaterial part of response does not correlate with the material part of the predictor. The regression coefficients are computed using only the material part of the response and predictor space. The number of components specified in both of these methods during the fit influence the separation of these spaces. If the number of response components equals to the number of responses, simultaneous envelope reduces to the predictor envelope and if the number of predictor components equals to the number of predictors, the result will be equivalent to the ordinary least squares. @cook2015simultaneous and @cook2018envelope have discussed the method in detail. Further, @helland2016algorithms have discussed how the population model of PCR, PLS and Xenv are equivalent.
