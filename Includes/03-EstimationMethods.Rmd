---
bibliography: 'references.bib'
---

# Estimation Methods #

Consider a joint distribution of $\mathbf{y}$ and $\mathbf{x}$ with corresponding mean vectors $\boldsymbol{\mu}_y$ and $\boldsymbol{\mu}_x$ as,

\begin{equation}
  \begin{bmatrix}
    \mathbf{y} \\ \mathbf{x}
  \end{bmatrix} 
  \sim \mathsf{N}
  \begin{pmatrix}
    \begin{bmatrix}
      \boldsymbol{\mu}_y \\
      \boldsymbol{\mu}_x \\
    \end{bmatrix}, &&
    \begin{bmatrix}
      \boldsymbol{\Sigma}_{yy} & \boldsymbol{\Sigma}_{yx} \\
      \boldsymbol{\Sigma}_{xy} & \boldsymbol{\Sigma}_{xx} 
    \end{bmatrix}
  \end{pmatrix}
  (\#eq:model-1)
\end{equation}

Here, $\boldsymbol{\Sigma}_{xx}$ and $\boldsymbol{\Sigma}_{yy}$ are variance-covariance of $\mathbf{x}$ and $\mathbf{y}$ respectively and $\boldsymbol{\Sigma}_{xy}=\boldsymbol{\Sigma}_{yx}^t$ is the covariance matrix of $\mathbf{x}$ and $\mathbf{y}$. Let $\mathbf{S}_{xx}$, $\mathbf{S}_{yy}$ and $\mathbf{S}_{xy}=\mathbf{S}_{yx}^t$ be the respective estimates of these matrices. A linear regression model based on \@ref(eq:model-1) is 

\begin{equation}
\mathbf{y} = \boldsymbol{\mu}_y + \boldsymbol{\beta}^t\left( \mathbf{x} - \boldsymbol{\mu}_{x} \right) + \boldsymbol{\varepsilon}
(\#eq:reg-model)
\end{equation}

where $\boldsymbol{\beta}=\boldsymbol{\Sigma}_{xx}^{-1}\boldsymbol{\Sigma}_{xy}$ is the regression coefficient that defines the relationship between $\mathbf{x}$ and $\mathbf{y}$. We can write the least square estimates of $\boldsymbol{\beta}$ using the sample estimates as $\boldsymbol{\hat{\beta}}=\mathbf{S}_{xx}^{-1}\mathbf{S}_{xy}$. Consider a transformation $\mathbf{z}=\mathbf{R^tx}$ with an $k \times p (k<p)$ orthogonal rotation matrix $\mathbf{R}$ with $k$ orthogonal columns. We can rewrite the linear regression model in terms of these transformed variable as in model \@ref(eq:reg-model).

\begin{equation}
\mathbf{y} = \boldsymbol{\mu}_y + \boldsymbol{\alpha}^t\left( \mathbf{z} - \boldsymbol{\mu}_{z} \right) + \boldsymbol{\tau}
(\#eq:latent-model)
\end{equation}

The ordinary least square estimate of regression coefficient in equation \@ref(eq:latent-model) is $\boldsymbol{\hat{\alpha}}=\boldsymbol{S}_{zz}^{-1}\boldsymbol{S}_{zy}=\left[\mathbf{R}^t\mathbf{S}_{xx}\mathbf{R}\right]^{-1}\mathbf{R}^t \mathbf{S}_{xy}$. The coefficient $\boldsymbol{\hat{\alpha}}$ can be transformed back to $\boldsymbol{\hat{\beta}}$ as $\boldsymbol{\hat{\beta}}=\mathbf{R}\boldsymbol{\hat{\alpha}}=\mathbf{R}\left[\mathbf{R}^t\mathbf{S}_{xx}\mathbf{R}\right]^{-1}\mathbf{R}^t \mathbf{S}_{xy}$. When $\mathbf{R}$ is a full-rank matrix, i.e. $k=p$ then, $\boldsymbol{\hat{\beta}}$ reduces to ordinary least square estimates $\mathbf{S}_{xx}^{-1}\mathbf{S}_{xy}$.

_Principal Components Regression_ (PCR) uses $k$ eigenvectors of $\mathbf{S}_{xx}$ as the columns of $\mathbf{R}$. Since PCR is based on capturing the maximum variation in predictors on every component it has used to model, this method does not consider the response structure [@Jolliffe2002]. In addition, if the relevant components are not in the initial position, the method requires more number of components to make precise prediction [@Alm_y_1996].


_Partial Least Squares_ (PLS) regression tries to maximize the covariance between the predictors and the response scores [@DeJong1993]. Broadly, PLS can be divided into PLS1 and PLS2 where the former tries to model the response variables individually and the later using all the response variable together while modelling. Among the three widely used algorithms NIPALS [@wold75nipals], SIMPLS [@DeJong1993] and KernelPLS [@Lindgren_1993], for this study we will be using KernelPLS which gives equivalent results to the other two and is default in R-package `pls` [@mevik07_thepl].


_Envelopes_ is first introduced by [@Cook2007a] as the smallest subspace that includes the span of true regression coefficients. _Predictor Envelopes_ (Xenv) identifies the envelope as a smallest subspace in predictor space by separating the predictor covariance $\boldsymbol{\Sigma}_{xx}$ into relevant (material) and irrelevant (immaterial) parts such that response $\mathbf{y}$ is uncorrelated with the irrelevant part given the relevant one. In addition, relevant and irrelevant parts are also uncorrelated. Such separation of the covariance matrix is made using the data through optimization of the objective function. Further, the regression coefficients are estimated only using the relevant part. @cook2010envelope, @cook2013envelopes and @cook2018envelope have extensively discussed the foundation and various mathematical constructs together with properties related to Predictor Envelope. 

_Simultaneous Predictor-Response Envelope_ (Senv) implement the envelope in both response and predictor spaces. It separates the material and immaterial part in response space and predictor space such that the material part of response does not correlate with the immaterial part of predictor and the immaterial part of response does not correlate with the material part of the predictor. The regression coefficients are computed using only the material part of the response and predictor space. The number of components specified in both of these methods during the fit influence the separation of these spaces. If the number of response components equals to the number of responses, simultaneous envelope reduces to the predictor envelope and if the number of predictor components equals to the number of predictors, the result will be equivalent to the ordinary least squares. @cook2015simultaneous and @cook2018envelope have discussed the method in detail. Further, @helland2016algorithms have discussed how the population model of PCR, PLS and Xenv are equivalent.
