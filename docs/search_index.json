[
["index.html", "Comparison of Multivariate Estimation Methods Introduction", " Comparison of Multivariate Estimation Methods Raju Rimal KBMraju.rimal@nmbu.no Trygve Almøy KBMtrygve.almoy@nmbu.no Solve Sæbø NMBUsolve.sabo@nmbu.no 2019-01-25 Abstract Prediction performance often does not reflect the estimation behaviour of a method. High error in estimation not necessarily results in high prediction error but can leads to an unreliable prediction when test data are in a different direction than the training data. In addition, the effect of a variable becomes unstable and can not be interpreted in such situations. Many research fields are more interested in these estimates than performing prediction. This study compares some newly-developed (envelope) and well-established (PLS, PCR) prediction methods using simulated data with specifically designed properties such as multicollinearity, correlation between multiple responses and position of principal components of predictor that are relevant for the response. This study aims to give some insight on these methods and help researcher to understand and use them for further study. Write some specifics from the results to show what we have found. Introduction Estimation of parameters in a regression model is an integral part in many research study. Research fields such as social science, econometric, psychology and medical study are more interested in measuring the impact of certain indicator or variable rather than performing prediction. Such studies has large influence in people’s perception and also help in policy making and decisions. Technology has facilitated researcher to collected large amount of data however often times, such data either contains irrelevant information or are highly collinear. Researchers are devising new estimators to extract information and identify their inter-relationship. Some estimators are robust towards fixing multicollinearity problem while some are targeted to model only the relevant information content in response variable. This study extends the (Rimal, Almøy, and Sæbø 2019) and compares some well established estimators such as Principal Components Analysis (PCA), Partial Least Squares (PLS) together with two new methods based on envelope estimation: Envelope estimation in predictor space (Xenv) (R Dennis Cook, Li, and Chiaromonte 2010) and simultaneous estimation of envelope (Senv) (R. Dennis Cook and Zhang 2015). The estimation process of these methods are discussed in [Methods] section. The comparison tests the estimation performance of these methods using multi-response simulated data from linear model with controlled properties. The properties includes the number of predictors, level of multicollinearity, correlation between different response variables and the position of relevant predictor components. These properties are explained in [Experimental Design] section together with the strategy behind the simulation and data model. References "],
["simulation-model.html", "Simulation Model", " Simulation Model Consider a model where the response vector \\((\\mathbf{y})\\) with \\(m\\) elements and predictor vector \\((\\mathbf{x})\\) with \\(p\\) elements follow a multivariate normal distribution as follows, \\[\\begin{equation} \\begin{bmatrix} \\mathbf{y} \\\\ \\mathbf{x} \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\boldsymbol{\\mu}_y \\\\ \\boldsymbol{\\mu}_x \\end{bmatrix}, \\begin{bmatrix} \\boldsymbol{\\Sigma}_{yy} &amp; \\boldsymbol{\\Sigma}_{yx} \\\\ \\boldsymbol{\\Sigma}_{xy} &amp; \\boldsymbol{\\Sigma}_{xx} \\end{bmatrix} \\right) \\tag{1} \\end{equation}\\] where, \\(\\boldsymbol{\\Sigma}_{yy}\\) and \\(\\boldsymbol{\\Sigma}_{xx}\\) are the variance-covariance matrices of \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\), respectively, \\(\\boldsymbol{\\Sigma}_{xy}\\) is the covariance between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) and \\(\\boldsymbol{\\mu}_y\\) and \\(\\boldsymbol{\\mu}_x\\) are mean vectors of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), respectively. A linear model based on (1) is, \\[\\begin{equation} \\mathbf{y} = \\boldsymbol{\\mu}_y + \\boldsymbol{\\beta}^t(\\mathbf{x} - \\boldsymbol{\\mu_x}) + \\boldsymbol{\\epsilon} \\tag{2} \\end{equation}\\] where, \\(\\underset{m\\times p}{\\boldsymbol{\\beta}^t}\\) is a matrix of regression coefficients and \\(\\boldsymbol{\\epsilon}\\) is an error term such that \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\boldsymbol{\\Sigma}_{y|x})\\) In a model like (2), we assume that the variation in response \\(\\mathbf{y}\\) is partly explained by the predictor \\(\\mathbf{x}\\). However, in many situations, only a subspace of the predictor space is relevant for the variation in the response \\(\\mathbf{y}\\). This space can be referred to as the relevant space of \\(\\mathbf{x}\\) and the rest as irrelevant space. In the similar manner, we can assume that a subset of the response space contains the information that the predictors can explain for a given model (Figure-1). R Dennis Cook, Li, and Chiaromonte (2010) and R. Dennis Cook and Zhang (2015) have referred to the relevant space as material space, and the irrelevant space as immaterial space. Figure 1: Relevant space in a regression model With an orthogonal transformation of \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\) to latent variables \\(\\mathbf{w}\\) and \\(\\mathbf{z}\\), respectively, by \\(\\mathbf{w=Qy}\\) and \\(\\mathbf{z = Rx}\\), where \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) are orthogonal rotation matrices, an equivalent model to (2) in terms of the latent variables can be written as, \\[\\begin{equation} \\mathbf{w} = \\boldsymbol{\\mu}_w + \\boldsymbol{\\alpha}^t(\\mathbf{z} - \\boldsymbol{\\mu_z}) + \\boldsymbol{\\tau} \\tag{3} \\end{equation}\\] where, \\(\\underset{m\\times p}{\\boldsymbol{\\alpha}^t}\\) is a matrix of regression coefficients and \\(\\boldsymbol{\\tau}\\) is an error term such that \\(\\boldsymbol{\\tau} \\sim \\mathcal{N}(0, \\boldsymbol{\\Sigma}_{w|z})\\). Model (3) follows the distribution, \\[\\begin{equation} \\begin{bmatrix} \\mathbf{w} \\\\ \\mathbf{z} \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\boldsymbol{\\mu}_w \\\\ \\boldsymbol{\\mu}_z \\end{bmatrix}, \\begin{bmatrix} \\boldsymbol{\\Sigma}_{ww} &amp; \\boldsymbol{\\Sigma}_{wz} \\\\ \\boldsymbol{\\Sigma}_{zw} &amp; \\boldsymbol{\\Sigma}_{zz} \\end{bmatrix} \\right) \\tag{4} \\end{equation}\\] where, \\(\\boldsymbol{\\Sigma}_{ww}\\) and \\(\\boldsymbol{\\Sigma}_{zz}\\) are the variance-covariance matrices of \\(\\mathbf{w}\\) and \\(\\mathbf{z}\\), respectively. \\(\\boldsymbol{\\Sigma}_{zw}\\) is the covariance between \\(\\mathbf{z}\\) and \\(\\mathbf{w}\\). \\(\\boldsymbol{\\mu}_w\\) and \\(\\boldsymbol{\\mu}_z\\) are mean vector of \\(\\mathbf{z}\\) and \\(\\mathbf{w}\\) respectively. Here, the elements of \\(\\mathbf{w}\\) and \\(\\mathbf{z}\\) are the principal components of responses and predictors, which will respectively be referred as “response components” and “predictor components”. The column vectors of respective rotation matrices \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) are the eigenvectors corresponding to these principal components. Following the concept of relevant space, a subset of predictor components can be imagined to span the predictor space. These components can be regarded as relevant predictor components. Naes and Martens (1985) introduced the concept of relevant components which was explored further by Inge S. Helland (1990), Næs and Helland (1993), Inge S. Helland and Almøy (1994) and Inge S. Helland (2000). The corresponding eigenvectors were referred to as relevant eigenvectors. A similar logic is introduced by R Dennis Cook, Li, and Chiaromonte (2010) and later by R. D. Cook, Helland, and Su (2013) as an envelope which is the space spanned by the relevant eigenvectors (R. Dennis Cook 2018, 101). In addition, various simulation studies have been performed with the model based on the concept of relevant subspace. A simulation study by Almøy (1996) has used a single response simulation model based on reduced regression and has compared some contemporary multivariate estimators. In the recent years Inge S. Helland, Saebø, and Tjelmeland (2012), S. Sæbø, Almøy, and Helland (2015), Inge Svein Helland et al. (2018) and Rimal, Almøy, and Sæbø (2018) implemented similar simulation examples as we are discussing in this study. This paper, however, presents an extensive simulation study based on multi-response data simulated with experimental design and compares relatively new methods such as simultaneous envelopes with well established methods such as partial least squares and principal components regression. Rimal, Almøy, and Sæbø (2018) has a detail discussion about the simulation model that we have opted here. The following section presents the estimators under comparison in more detail. References "],
["prediction-methods.html", "Prediction Methods Modification in envelope estimation", " Prediction Methods Partial least squares regression (PLS) and Principal component regression (PCR) has been used in many disciplines such as chemometrics, econometrics, bioinformatics and machine learning, where wide predictor matrices, i.e. \\(p\\) (number or predictors) &gt; \\(n\\) (number of observation) is common. These methods are popular in multivariate analysis, especially for exploratory studies and prediction. In recent years, a concept of envelope introduced by R. Dennis Cook, Li, and Chiaromonte (2007) based on reduction in regression model has been implemented for the development of envelope estimation in the subsequent papers. In this study, we will follow estimation methods based on their prediction performance on data simulated with different controlled properties. Principal Components Regression (PCR): Principal components are the linear combinations of predictor variables such that the transformation makes the new variables uncorrelated and the variation of the original dataset captured by them are ordered. In other words, each successive components captures maximum variation left by the preceding components in predictor variables (Jolliffe 2002). Principal components regression uses these principal components to explain the variation in the response. Partial Least Squares (PLS): Two variants of PLS: PLS1 and PLS2 will be used for comparison. The first one considers individual response variables separately, i.e. each response is predicted with a single response model, while the latter considers all response variables together. In PLS regression the components are determined such as to maximize a covariance between response and predictors (Jong 1993). Envelopes: The envelope, introduced by R. Dennis Cook, Li, and Chiaromonte (2007), was first used as a response envelope (R Dennis Cook, Li, and Chiaromonte 2010) as a smallest subspace \\(\\mathcal{E}\\) in the response space such that the span of regression coefficients lies in that space. Since a multivariate linear regression model contains relevant (material) and irrelevant (immaterial) variation in both response and predictor, the relevant part provides information, while irrelevant part increases the estimative variation. The concept of envelope uses the relevant part for estimation while excluding the irrelevant part consequently increasing the efficiency of the model (R. Dennis Cook and Zhang 2016). The concept was later extended to the predictor space, where the predictor envelope was defined (R. D. Cook, Helland, and Su 2013). Further R. Dennis Cook and Zhang (2015) uses envelopes for joint reduction of the responses and predictors and argued to produce efficiency gains greater than using individual envelops either of the response and predictors. All the variants of envelope estimations are based on maximum likelihood estimation. Here in this study we will also use predictor envelope (Xenv) and simultaneous envelope (Senv) for the comparison. Modification in envelope estimation Since envelope estimators (Xenv and Senv) are based on maximum likelihood estimation (MLE), it fails to estimate in case of wide matrices, i.e. \\(p &gt; n\\). In order to incorporate these methods in our comparison, we have used the principal components \\((\\mathbf{z})\\) of the predictor variables \\((\\mathbf{x})\\) as predictors, using the required number of components for capturing 97.5% of the variation in \\(\\mathbf{x}\\). The new set of variables, \\(\\mathbf{z}\\), were used for envelope estimation. The regression coefficients \\((\\hat{\\boldsymbol{\\alpha}})\\) corresponding to these new variables \\(\\mathbf{z}\\) were transformed back to obtain coefficients for each predictor variable as, \\[\\hat{\\boldsymbol{\\beta}} = \\mathbf{e}_k\\hat{\\boldsymbol{\\alpha}_k}\\] where, \\(\\mathbf{e}_k\\) is the eigenvectors with \\(k\\) number of components. "]
]
