@article{gangsei2016linear,
  author =       {Lars Erik Gangsei and Trygve Almøy and Solve Sæbø},
  title =        {Linear regression with bivariate response variable containing
                  missing data. Strategies to increase prediction precision},
  journal =      {Communications in Statistics - Simulation and Computation},
  volume =       0,
  number =       0,
  pages =        {1-12},
  year =         2019,
  publisher =    {Taylor & Francis},
  doi =          {10.1080/03610918.2019.1656249},
  URL =          {https://doi.org/10.1080/03610918.2019.1656249},
  abstract =     { AbstractIn this study we focus on prediction precision for
                  linear regression with a bivariate response variable, where
                  the response variable of primary interest contains missing
                  data in the training data set. We derive and provide the
                  maximum likelihood solution and a Bayesian method based on a
                  conjugate prior distribution. In particular we evaluate
                  strategies in how to “borrow prediction strength” from the
                  full set of data to prediction associated with the missing
                  data. Regularization of the maximum likelihood estimator is
                  theoretically shown to be beneficial, and we derive methods
                  for how to implement such regularization under frequentist and
                  Bayesian inference, including available software as a
                  R-package. }
}

@article{helland2016algorithms,
  abstract =     {Partial least squares regression has been a very popular
                  method for prediction, first used by chemometricians, but also
                  now by statisticians in a number of applied fields. The method
                  can in a natural way be connected to a statistical model which
                  now has been ex- tended and further developed in terms of an
                  envelope model. Concentrating on the uni- variate case, the
                  model is here approached from several points of view. Several
                  estimators of the regression vector in this model are defined,
                  including the ordinary PLS estimator, the maximum likelihood
                  envelope estimator and a recently proposed Bayes PLS
                  estimator. The different estimators are compared with respect
                  to prediction error by systematic sim- ulations. The main
                  conclusion from the simulations seems to be that Bayes PLS
                  performs well compared to the other methods in virtually all
                  cases.},
  annote =       {From Duplicate 1 (Model and estimators for partial least
                  squares - Helland, Inge S.; S{\ae}b{\o}, Solve; Alm{\o}y,
                  Trygve; Rimal, Raju) From Duplicate 1 (Model and estimators
                  for partial least squares - Helland, Inge S; S{\ae}b{\o}, S;
                  Alm{\o}y, T; Rimal, R) unpublished},
  author =       {Helland, Inge Svein and Saeb{\o}, Solve and Alm{\o}y, Trygve
                  and Rimal, Raju},
  doi =          {10.1002/cem.3044},
  file =         {:Users/therimalaya/Dropbox/Papers/References//Helland et
                  al.{\_}2018{\_}Model and estimators for partial least squares
                  regression.pdf:pdf},
  issn =         08869383,
  journal =      {Journal of Chemometrics},
  keywords =     {Data Simulation,Expected Prediction error,Experimental
                  Design,Linear Model,Partial Least Squares
                  Regression,Prediction Ability,Principal Component
                  Regression,R-Package,bayes pls estimator,envelope
                  model,maximum likelihood envelope estima-,partial least
                  squares,partial least squares model,prediction,relevant
                  Components,simulation,tor},
  month =        {sep},
  number =       9,
  pages =        {e3044},
  publisher =    {Wiley Online Library},
  title =        {{Model and estimators for partial least squares regression}},
  volume =       32,
  year =         2018
}

@article{helland2012near,
  author =       {Inge, Helland S. and S{\ae}b{\o}, Solve and Tjelmeland, H{\aa}kon},
  title =        {Near Optimal Prediction from Relevant Components},
  journal =      {Scandinavian Journal of Statistics},
  volume =       39,
  number =       4,
  pages =        {695-713},
  keywords =     {Bayesian estimation, envelope model, equivariance, group,
                  invariant measure, Markov chain Monte Carlo, partial least
                  squares model, prediction, relevant components},
  doi =          {10.1111/j.1467-9469.2011.00770.x},
  url =
                  {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2011.00770.x},
  abstract =     {Abstract.  The random x regression model is approached through
                  the group of rotations of the eigenvectors for the
                  x-covariance matrix together with scale transformations for
                  each of the corresponding regression coefficients. The partial
                  least squares model can be constructed from the orbits of this
                  group. A generalization of Pitman's Theorem says that the best
                  equivariant estimator under a group is given by the Bayes
                  estimator with the group's invariant measure as the prior. A
                  straightforward application of this theorem turns out to be
                  impossible since the relevant invariant prior leads to a
                  non-defined posterior. Nevertheless we can devise an
                  approximate scale group with a proper invariant prior leading
                  to a well-defined posterior distribution with a finite mean.
                  This Bayes estimator is explored using Markov chain Monte
                  Carlo technique. The estimator seems to require heavy
                  computations, but can be argued to have several nice
                  properties. It is also a valid estimator when p>n.},
  year =         2012
}
